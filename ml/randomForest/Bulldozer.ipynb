{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulldozers example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize & install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install sklearn\n",
    "!pip install torch\n",
    "!pip install nvidia-ml-py3 beautifulsoup4 fastprogress\n",
    "!pip install bottleneck dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from fastai.tabular import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 'Blue book Bulldozer' dataset from Kallgle\n",
    "data_path = 'data/bulldozer/TrainAndValid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When loading, make sure we parse 'saledate' column\n",
    "df_raw = pd.read_csv(data_path, low_memory=False, parse_dates=['saledate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use log of salesprice instead of saleprice (as defined in the problem description)\n",
    "df_ori.SalePrice = np.log(df_ori.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "sanitize_valid_chars = set('-_.abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "\n",
    "def ifnone(val, val_default):\n",
    "    \"`a` if `a` is not None, otherwise `b`. Source: fast.ai\"\n",
    "    return val_default if val is None else val\n",
    "\n",
    "def sanitize(s):\n",
    "    ''' Sanitize a string by only allowing \"valid\" characters '''\n",
    "    return ''.join(c for c in str(s) if c in sanitize_valid_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "class DfPreproc:\n",
    "    ''' \n",
    "    Data Frame preprocessing. Calculate some field (i.e. dataframe column) transformations\n",
    "    and store the results in 'columns_to_add' and 'columns_to_remove'. Then apply these\n",
    "    changes to the original dataframe to create a new dataframe.    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, df:DataFrame, columns_date=None, one_hot_max_cardinality:int=7):\n",
    "        '''\n",
    "        Parameters:\n",
    "            df : dataFrame\n",
    "            columns_date: Columns to be expanded into multiple date/time fields\n",
    "            one_hot_max_cardinality: Max cardinality (i.e. number of categories) for 'one hot' encoding.\n",
    "        '''\n",
    "        self.df = df\n",
    "        self.one_hot_max_cardinality = one_hot_max_cardinality\n",
    "        self.columns_date = ifnone(columns_date, set())\n",
    "        self.df_new = None\n",
    "        self.categories = dict()\n",
    "        self.columns_to_add = dict()\n",
    "        self.columns_to_remove = set()\n",
    "\n",
    "    def add_datepart(self, field_name:str, prefix:str=None, time:bool=True):\n",
    "        '''\n",
    "        Helper function that creates a new dataframe with all columns relevant to\n",
    "        a date in the column `field_name`.\n",
    "        Source: fast.ai \n",
    "        '''\n",
    "        make_date(self.df, field_name)\n",
    "        field = self.df[field_name]\n",
    "        prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n",
    "        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start', \n",
    "                'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        if time:\n",
    "            attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        df = pd.DataFrame()\n",
    "        for n in attr:\n",
    "            df[prefix + n] = getattr(field.dt, n.lower())\n",
    "        df[prefix + 'Elapsed'] = field.astype(np.int64) // 10 ** 9\n",
    "        # Add to replace and remove operations\n",
    "        self.columns_to_add[field_name] = df\n",
    "        self.columns_to_remove.add(field_name)\n",
    "\n",
    "    def category(self, field_name:str):\n",
    "        \" Convert field to category numbers \"\n",
    "        xi = self.df[field_name]\n",
    "        xi_cat = xi.astype('category').cat.as_ordered()\n",
    "        self.categories[field_name] = xi_cat\n",
    "        df_cat = DataFrame()\n",
    "        df_cat[field_name] = xi_cat.cat.codes\n",
    "        # Add to replace and remove operations\n",
    "        self.columns_to_add[field_name] = df_cat\n",
    "        self.columns_to_remove.add(field_name)\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\" Create a new dataFrame based on the previously calculated conversions \"\"\"\n",
    "        # Create new dataFrame\n",
    "        df_new = self.df.copy()\n",
    "        # Drop old columns categorical columns\n",
    "        df_new.drop(list(self.columns_to_remove), axis=1, inplace=True)\n",
    "        # Join new columns\n",
    "        for c in self.columns_to_add:\n",
    "            df_new = df_new.join(self.columns_to_add[c])\n",
    "        self.df_new = df_new\n",
    "        return df_new\n",
    "\n",
    "    def make_date(self, df:DataFrame, date_field:str):\n",
    "        \" Make sure `df[field_name]` is of the right date type. Source: fast.ai \"\n",
    "        field_dtype = df[date_field].dtype\n",
    "        if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            field_dtype = np.datetime64\n",
    "        if not np.issubdtype(field_dtype, np.datetime64):\n",
    "            df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n",
    "\n",
    "    def na(self, field_name:str):\n",
    "        \" Process 'na' columns (i.e missing data) \"\n",
    "        # Add '*_na' column\n",
    "        xi = self.df[field_name].copy()\n",
    "        df_na = DataFrame()\n",
    "        df_na[f\"{field_name}_na\"] = xi.isna().astype('int8')\n",
    "        # Replace missing values by median\n",
    "        replace_value = xi.median()\n",
    "        xi[xi.isna()] = replace_value\n",
    "        df_na[field_name] = xi\n",
    "        # Add operations\n",
    "        self.columns_to_add[field_name] = df_na\n",
    "        self.columns_to_remove.add(field_name)\n",
    "\n",
    "    def new(self):\n",
    "        \" Preprocess and create a new dataframe from pre-processing transformations \"\n",
    "        self.preprocess()\n",
    "        return self.create()\n",
    "\n",
    "    def one_hot(self, field_name:str, has_na:bool):\n",
    "        \" Create a one hot encodig for 'field_name' \"\n",
    "        df_one_hot = pd.get_dummies(self.df[field_name], dummy_na=has_na)\n",
    "        self.rename_category_cols(df_one_hot, f\"{field_name}_\")\n",
    "        # Add to transformations\n",
    "        self.columns_to_add[field_name] = df_one_hot\n",
    "        self.columns_to_remove.add(field_name)\n",
    "\n",
    "    def preprocess(self):\n",
    "        '''\n",
    "        Perform data frame pre-processing steps:\n",
    "            - Convert categorical data\n",
    "            - Convert on-hot encoding\n",
    "            - Convert dates into multiple columns\n",
    "        \n",
    "        '''\n",
    "        for c in self.df.columns:\n",
    "            xi = self.df[c]\n",
    "            has_na = (xi.isna().sum() > 0)\n",
    "\n",
    "            if c in self.columns_date:\n",
    "                self.add_datepart(c)\n",
    "                if has_na:\n",
    "                    print(f\"{c}: date part + NA {xi.dtype}\")\n",
    "                    self.na(c)\n",
    "                else:\n",
    "                    print(f\"{c}: date part\")\n",
    "            elif xi.dtype == 'O':\n",
    "                values = xi.unique()\n",
    "                count_cats = len(values)\n",
    "                # Create transformations to convert field (column)\n",
    "                if count_cats <= self.one_hot_max_cardinality:\n",
    "                    print(f\"{c}: one hot\")\n",
    "                    self.one_hot(c, has_na)\n",
    "                elif pd.api.types.is_string_dtype(xi):\n",
    "                    print(f\"{c}: category\")\n",
    "                    self.category(c)\n",
    "                else:\n",
    "                    print(f\"{c}: ok {xi.dtype}\")                    \n",
    "            else:\n",
    "                if has_na:\n",
    "                    print(f\"{c}: NA {xi.dtype}\")\n",
    "                    self.na(c)\n",
    "                else:\n",
    "                    print(f\"{c}: ok {xi.dtype}\")\n",
    "\n",
    "    def rename_category_cols(self, df, prepend):\n",
    "        '''\n",
    "        Rename dataFrame columns by prepending a string and sanitizing the name\n",
    "        Used to rename columns of a 'one hot' encoding\n",
    "        '''\n",
    "        names = dict()\n",
    "        for c in df.columns:\n",
    "            name = f\"{prepend}{sanitize(c)}\"\n",
    "            names[c] = name\n",
    "        df.rename(columns=names, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpp = DfPreproc(df_ori, columns_date=['saledate'])\n",
    "df = dfpp.new()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.drop(dep_var, axis=1), df[dep_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-sample training set for speed\n",
    "train_subset = 50_000\n",
    "validate_subset = 12_000\n",
    "\n",
    "tv = train_subset + validate_subset\n",
    "x_train = x[-tv:-validate_subset]\n",
    "y_train = y[-tv:-validate_subset]\n",
    "\n",
    "# Validate\n",
    "x_val = x[-validate_subset:]\n",
    "y_val = y[-validate_subset:]\n",
    "\n",
    "print(f\"Trainig set: {len(x_train)}\\tValidation set:{len(x_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to show training results\n",
    "\n",
    "def rmse(x, y): return math.sqrt(((x - y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    ret = [rmse(m.predict(x_train), y_train), rmse(m.predict(x_val), y_val), m.score(x_train, y_train), m.score(x_val, y_val)]\n",
    "    print(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with a single tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model: Zero (exploratory analysis)\n",
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=1, bootstrap=False)\n",
    "m.fit(x_train, y_train)\n",
    "print_score(m)\n",
    "\n",
    "# Note: We should fit perfectly the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with single (shallow) tree for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model: Zero (exploratory analysis)\n",
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=1, max_depth=3, bootstrap=False)\n",
    "m.fit(x_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the tree to a graphviz 'dot' format\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython import display\n",
    "\n",
    "str_tree = export_graphviz(m.estimators_[0],\n",
    "                           out_file='tree.dot',\n",
    "                           feature_names=x_train.columns,\n",
    "                           filled=True,\n",
    "                           rounded=True)\n",
    "\n",
    "# Convert 'dot' to 'png'\n",
    "!dot -Tpng 'tree.dot' -o 'tree.png'\n",
    "\n",
    "# Show image\n",
    "from IPython.display import Image\n",
    "Image(filename='tree.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 10 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=10)\n",
    "%time m.fit(x_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with  more trees and more randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=40, min_samples_leaf=3, max_features=0.5)\n",
    "%time m.fit(x_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfidence:\n",
    "    ''' Calculate mean and variance of the estimation base on multiple modes '''\n",
    "    \n",
    "    def __init__(self, models, max_workers=None):\n",
    "        '''\n",
    "        Models can be either a is a list or an Ensembl estimator\n",
    "        from SciKitLearn (e.g. a ranfdom forest)\n",
    "        '''\n",
    "        self.models = models.estimators_ if 'estimators_' in models else models\n",
    "        self.max_workers = max_workers\n",
    "        self.predictions = None\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' Predict using all models, return mean prediction '''\n",
    "        self.predictions = np.stack([m.predict(x) for m in self.models])\n",
    "        return self.predictions.mean(axis=0)\n",
    "\n",
    "    def predict_single_sample(self, x, sample_number):\n",
    "        ''' Predict using all models, return mean prediction for sample number '''\n",
    "        xi = x_val.iloc[sample_number]\n",
    "        xi = np.array(xi).reshape(1,-1)\n",
    "        return self.predict(x)\n",
    "\n",
    "    def std(self):\n",
    "        \" Standard deviation of previous predictions \"\n",
    "        return self.predictions.std(axis=0)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"number of models: {len(self.models)}, mean : {np.mean(self.predictions)}, std: {np.std(self.predictions)}, predictions: {self.predictions}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelConfidence(m)\n",
    "\n",
    "%time preds = mc.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(preds, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mc.std(), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy as hc\n",
    "\n",
    "class FeatureImportance:\n",
    "    '''\n",
    "    Estimate feature importance.\n",
    "    How it works: Suffle a column and analyze how model performance is\n",
    "    degraded. Most important features will make the model perform much\n",
    "    worse when shuffled, unimportant features will not affect performance\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model, x, y):\n",
    "        self.model = model\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.performance = dict()\n",
    "        self.importance = None\n",
    "        self.figsize = (16, 10)\n",
    "\n",
    "    def analyze(self, verbose=False):\n",
    "        # Base performance\n",
    "        x_copy = self.x.copy()\n",
    "        pred = self.model.predict(x_copy)\n",
    "        # Shuffle each solumn\n",
    "        for c in self.x:\n",
    "            # Shuffle column 'c'\n",
    "            x_copy = self.x.copy()\n",
    "            xi = np.random.permutation(x_copy[c])\n",
    "            x_copy[c] = xi\n",
    "            # How did it perform\n",
    "            pred_c = self.model.predict(x_copy)\n",
    "            perf_c = self.rmse(pred, pred_c)\n",
    "            self.performance[c] = perf_c\n",
    "            if verbose:\n",
    "                print(f\"{c}: {perf_c}\")\n",
    "        # List of items sorted by importance (most important first)\n",
    "        self.importance = sorted(self.performance.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    def analyze_pairs(self, threshold, verbose=False):\n",
    "        # Base performance\n",
    "        x_copy = self.x.copy()\n",
    "        pred = self.model.predict(x_copy)\n",
    "        self.performance2 = dict()\n",
    "        # Shuffle each solumn\n",
    "        for i1 in self.importance:\n",
    "            c1, i1_perf = i1[0], i1[1]\n",
    "            if i1_perf < threshold:\n",
    "                continue\n",
    "            for i2 in self.importance:\n",
    "                c2, i2_perf = i2[0], i2[1]\n",
    "                if i2_perf < threshold or i1_perf < i2_perf or c1 == c2:\n",
    "                    continue\n",
    "                # Shuffle columns 'c1' and 'c2'\n",
    "                x_copy = self.x.copy()\n",
    "                xi1 = np.random.permutation(x_copy[c1])\n",
    "                xi2 = np.random.permutation(x_copy[c2])\n",
    "                x_copy[c1] = xi1\n",
    "                x_copy[c2] = xi2\n",
    "                # How did it perform\n",
    "                pred_c = self.model.predict(x_copy)\n",
    "                perf_c = self.rmse(pred, pred_c)\n",
    "                if perf_c > (i1_perf + i2_perf):\n",
    "                    print(f\"'{c1}' + '{c2}': {perf_c}\\t<- DOUBLE\")\n",
    "                    self.performance2[f\"{c1}\\t{c2}\"] = perf_c\n",
    "                elif perf_c > max(i1_perf, i2_perf):\n",
    "                    print(f\"'{c1}' + '{c2}': {perf_c}\\t<- SINGLE\")\n",
    "                    self.performance2[f\"{c1}\\t{c2}\"] = perf_c\n",
    "                elif verbose:\n",
    "                    print(f\"'{c1}' + '{c2}': {perf_c}\")\n",
    "                    \n",
    "        # List of items sorted by importance (most important first)\n",
    "        self.importance2 = sorted(self.performance2.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    def most_important(self, importance_threshold=None, ratio_to_most_important=100, df=None):\n",
    "        \"\"\"\n",
    "        Select features to keep either using an absolute value or\n",
    "        a ratio to most important feature\n",
    "        \"\"\"\n",
    "        if ratio_to_most_important is not None:\n",
    "            most_important = fi.importance[0]\n",
    "            importance_threshold = most_important[1] / ratio_to_most_important\n",
    "            \n",
    "        important_features = [f[0] for f in self.importance if f[1] > importance_threshold]\n",
    "        unimportant_features = [f[0] for f in self.importance if f[0] not in important_features]\n",
    "        return important_features, unimportant_features\n",
    "        \n",
    "    def plot(self, x=None):\n",
    "        \" Plot importance distributions \"\n",
    "        imp_x = np.array([f[0] for f in self.importance])\n",
    "        imp_y = np.array([f[1] for f in self.importance])\n",
    "        # Show bar plot\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        plt.barh(imp_x, imp_y)\n",
    "        # Show line plot\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        plt.plot(imp_x, imp_y)\n",
    "\n",
    "    def rmse(self, x, y):\n",
    "        return math.sqrt(((x - y)**2).mean())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([f\"{f[0]} : {f[1]}\" for f in fi.importance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = FeatureImportance(m, x_val, y_val)\n",
    "%time fi.analyze(verbose=True)\n",
    "fii = fi.importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2 = FeatureImportance(m, x_val, y_val)\n",
    "fi2.importance = fii\n",
    "%time fi2.analyze_pairs(threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi2.importance2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to keep\n",
    "imp, unimp = fi.most_important(ratio_to_most_important=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imp) / len(fi.importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(labels=unimp, axis=1, inplace=True)\n",
    "x_val.drop(labels=unimp, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1, n_estimators=40, min_samples_leaf=3)\n",
    "%time m.fit(x_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = FeatureImportance(m, x_val, y_val)\n",
    "%time fi.analyze()\n",
    "print(fi)\n",
    "fi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalesID: ok int64\n",
      "SalePrice: ok float64\n",
      "MachineID: ok int64\n",
      "ModelID: ok int64\n",
      "datasource: ok int64\n",
      "auctioneerID: NA float64\n",
      "YearMade: ok int64\n",
      "MachineHoursCurrentMeter: NA float64\n",
      "UsageBand: category\n",
      "saledate: date part\n",
      "fiModelDesc: category\n",
      "fiBaseModel: category\n",
      "fiSecondaryDesc: category\n",
      "fiModelSeries: category\n",
      "fiModelDescriptor: category\n",
      "ProductSize: category\n",
      "fiProductClassDesc: category\n",
      "state: category\n",
      "ProductGroup: category\n",
      "ProductGroupDesc: category\n",
      "Drive_System: category\n",
      "Enclosure: category\n",
      "Forks: category\n",
      "Pad_Type: category\n",
      "Ride_Control: category\n",
      "Stick: category\n",
      "Transmission: category\n",
      "Turbocharged: category\n",
      "Blade_Extension: category\n",
      "Blade_Width: category\n",
      "Enclosure_Type: category\n",
      "Engine_Horsepower: category\n",
      "Hydraulics: category\n",
      "Pushblock: category\n",
      "Ripper: category\n",
      "Scarifier: category\n",
      "Tip_Control: category\n",
      "Tire_Size: category\n",
      "Coupler: category\n",
      "Coupler_System: category\n",
      "Grouser_Tracks: category\n",
      "Hydraulics_Flow: category\n",
      "Track_Type: category\n",
      "Undercarriage_Pad_Width: category\n",
      "Stick_Length: category\n",
      "Thumb: category\n",
      "Pattern_Changer: category\n",
      "Grouser_Type: category\n",
      "Backhoe_Mounting: category\n",
      "Blade_Type: category\n",
      "Travel_Controls: category\n",
      "Differential_Type: category\n",
      "Steering_Controls: category\n"
     ]
    }
   ],
   "source": [
    "dfpp = DfPreproc(df_ori, one_hot_max_cardinality=-1, columns_date=['saledate'])\n",
    "df = dfpp.new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig set: 50000\tValidation set:12000\n"
     ]
    }
   ],
   "source": [
    "x, y = df.drop(dep_var, axis=1), df[dep_var]\n",
    "\n",
    "# Sub-sample training set for speed\n",
    "train_subset = 50_000\n",
    "validate_subset = 12_000\n",
    "\n",
    "tv = train_subset + validate_subset\n",
    "x_train = x[-tv:-validate_subset]\n",
    "y_train = y[-tv:-validate_subset]\n",
    "\n",
    "# Validate\n",
    "x_val = x[-validate_subset:]\n",
    "y_val = y[-validate_subset:]\n",
    "\n",
    "print(f\"Trainig set: {len(x_train)}\\tValidation set:{len(x_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dendo(df):\n",
    "    to_drop = list()\n",
    "    for c in df.columns:\n",
    "        xi = df[c]\n",
    "        if xi.max() == xi.min():\n",
    "            to_drop.append(c)\n",
    "            print(f\"{c}\\t{xi.max()}\\t{xi.min()}\")\n",
    "    x = df.copy()\n",
    "    x.drop(to_drop, axis=1, inplace=True)\n",
    "    x = x.to_numpy()\n",
    "    sr = scipy.stats.spearmanr(x)\n",
    "    corr = sr.correlation\n",
    "    dist = 1 - corr\n",
    "    corr_condensed = hc.distance.squareform(dist)\n",
    "    z = hc.linkage(corr_condensed, method='average')\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    dendrogram(Z, labels = df.columns, orientation='left', leaf_font_size=16)\n",
    "    plt.show()\n",
    "\n",
    "# corr[np.isnan(corr)] = 0\n",
    "# np.fill_diagonal(corr, 1.0)\n",
    "\n",
    "dist = 1 - corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saleIs_year_end\tFalse\tFalse\n",
      "saleIs_year_start\tFalse\tFalse\n",
      "saleHour\t0\t0\n",
      "saleMinute\t0\t0\n",
      "saleSecond\t0\t0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Distance matrix 'X' must be symmetric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-2ba037a1edd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdendo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-5d71d9f8f78e>\u001b[0m in \u001b[0;36mdendo\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcorr_condensed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquareform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_condensed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'average'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36msquareform\u001b[0;34m(X, force, checks)\u001b[0m\n\u001b[1;32m   2172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The matrix argument must be square.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchecks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2174\u001b[0;31m             \u001b[0mis_valid_dm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;31m# One-side of the dimensions is set here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mis_valid_dm\u001b[0;34m(D, tol, throw, name, warning)\u001b[0m\n\u001b[1;32m   2248\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                     raise ValueError(('Distance matrix \\'%s\\' must be '\n\u001b[0;32m-> 2250\u001b[0;31m                                      'symmetric.') % name)\n\u001b[0m\u001b[1;32m   2251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distance matrix must be symmetric.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Distance matrix 'X' must be symmetric."
     ]
    }
   ],
   "source": [
    "dendo(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcingola/.local/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/pcingola/.local/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/home/pcingola/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/home/pcingola/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/home/pcingola/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=nan, pvalue=nan)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = x_train.iloc[:, 0].to_numpy()\n",
    "c2 = x_train.iloc[:, 20].to_numpy()\n",
    "\n",
    "scipy.stats.spearmanr(c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.min(), c2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Distance matrix 'X' must be symmetric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-2432e7865da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr_condensed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquareform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'average'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdendrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36msquareform\u001b[0;34m(X, force, checks)\u001b[0m\n\u001b[1;32m   2172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The matrix argument must be square.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchecks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2174\u001b[0;31m             \u001b[0mis_valid_dm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;31m# One-side of the dimensions is set here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py\u001b[0m in \u001b[0;36mis_valid_dm\u001b[0;34m(D, tol, throw, name, warning)\u001b[0m\n\u001b[1;32m   2248\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m                     raise ValueError(('Distance matrix \\'%s\\' must be '\n\u001b[0;32m-> 2250\u001b[0;31m                                      'symmetric.') % name)\n\u001b[0m\u001b[1;32m   2251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Distance matrix must be symmetric.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Distance matrix 'X' must be symmetric."
     ]
    }
   ],
   "source": [
    "corr_condensed = hc.distance.squareform(dist)\n",
    "z = hc.linkage(corr, method='average')\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "dendrogram(Z, labels = df.columns, orientation='left', leaf_font_size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-linear variables: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-129981f82a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaleElapsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train.saleElapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "InOut = collections.namedtuple('InOut', ['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = InOut(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InOut(x=None, y=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
