{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene2Vec: Pytorch\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "https://discuss.pytorch.org/t/what-kind-of-loss-is-better-to-use-in-multilabel-classification/32203\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tcga.msigdb import *\n",
    "from tcga.util import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names2num(names):\n",
    "    \"\"\" Create a mapping from names to numbers \"\"\"\n",
    "    names = list(set(names))  # Make sure names are unique\n",
    "    return {n: i for i,n in enumerate(sorted(names))}\n",
    "    \n",
    "class DatasetMsigDb(Dataset):\n",
    "    \"\"\" \n",
    "    Custom dataset: We have to override methods __len__ and __getitem__\n",
    "    In our network, the inputs are genes and the outputs are gene-sets.\n",
    "    We convert genes and gene sets to numbers, then store the forward and\n",
    "    reverse mappings.\n",
    "    \n",
    "    Genes: One hot encoding\n",
    "    \n",
    "    Gene sets: We encode gene-> gene_sets as a dictionary indexed by\n",
    "    gene, with tensors having 1 on the GeneSets the gene belongs to.\n",
    "    \n",
    "    The method __getitem__ returns a tuple with the gene (one-hot\n",
    "    encoded) and the gene-set tensor (having ones on all gene-sets\n",
    "    the gene belongs to)\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.msigdb = read_msigdb_all(path)\n",
    "        # Gene <-> number: forward and reverse mapping\n",
    "        self.gene2num = names2num(msigdb2genes(self.msigdb))\n",
    "        self.num2gene = {n: g for g, n in self.gene2num.items()}\n",
    "        # GeneSet <-> number: forward and reverse mapping\n",
    "        self.geneset2num = names2num(msigdb2gene_sets(self.msigdb))\n",
    "        self.num2geneset = {n: g for g, n in self.gene2num.items()}\n",
    "        # Gene -> GeneSets mapping (use gene_set numbers, in a tensor)\n",
    "        self.init_genes()\n",
    "        self.init_genesets()\n",
    "\n",
    "    def genesets2num(self, genesets):\n",
    "        \" Convert to a list of numerically encoded gene-sets \"\n",
    "        return [self.geneset2num[gs] for gs in genesets]\n",
    "\n",
    "    def gene2tensor(self, gene):\n",
    "        \" Convert to an index tensor (yes, it's just a number) \"\n",
    "        gene_tensor = torch.LongTensor([self.gene2num[gene]])\n",
    "        return gene_tensor\n",
    "        \n",
    "    def genesets2tensor(self, genesets):\n",
    "        \" Convert to a vector having 1 in each geneset position \"\n",
    "        geneset_idxs = [self.geneset2num[gs] for gs in genesets]\n",
    "        geneset_tensor = torch.zeros(len(self.msigdb))\n",
    "        geneset_tensor[geneset_idxs] = 1\n",
    "        return geneset_tensor\n",
    "        \n",
    "    def init_genes(self):\n",
    "        \" Create a one-hot encoding for a gene \"\n",
    "        self.gene_tensors = dict()\n",
    "        for gene in self.gene2num.keys():\n",
    "            self.gene_tensors[gene] = self.gene2tensor(gene)\n",
    "        \n",
    "    def init_genesets(self):\n",
    "        \" Map Gene to GeneSets. GeneSets are hot-encoded \"\n",
    "        self.gene_genesets = dict()\n",
    "        self.gene_genesets_num = dict()\n",
    "        self.gene_genesets_tensors = dict()\n",
    "        num_genesets = len(self.geneset2num)\n",
    "        for gene, genesets in gene_genesets(self.msigdb).items():\n",
    "            self.gene_genesets[gene] = genesets\n",
    "            self.gene_genesets_num[gene] = self.genesets2num(genesets)\n",
    "            self.gene_genesets_tensors[gene] = self.genesets2tensor(genesets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \" Len: Count number of genes \"\n",
    "        return len(self.gene2num)\n",
    "\n",
    "    def gene_sets_size(self):\n",
    "        \" Count number of gene sets \"\n",
    "        return len(self.msigdb)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \" Get item 'idx': A tuple of gene number 'idx' and gene set tensor for that gene \"\n",
    "        gene = self.num2gene[idx]\n",
    "        return (self.gene_tensors[gene], self.gene_genesets_tensors[gene])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        \" Show (a few) mappings gene -> gene_set tensor \"\n",
    "        out = f\"Genes: {len(self.gene2num)}, Gene Sets: {len(self.geneset2num)}\\n\"\n",
    "        for i in range(10):  #range(len(self)):\n",
    "            gene = self.num2gene[i]\n",
    "            gene_tensor, geneset_tensor = self[i]\n",
    "            out += f\"\\tGene: {gene}, {i}, {gene_tensor}\\n\\tGeneSet: {self.gene_genesets[gene]}, {self.gene_genesets_num[gene]}, {geneset_tensor}\\n\\n\"\n",
    "        return out + \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/msigdb/small/h.all.v7.0.symbols.gmt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Genes: 4384, Gene Sets: 50\n",
       "\tGene: A2M, 0, tensor([], dtype=torch.int64)\n",
       "\tGeneSet: {'HALLMARK_IL6_JAK_STAT3_SIGNALING', 'HALLMARK_COAGULATION'}, [23, 9], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AAAS, 1, tensor([140000886135904])\n",
       "\tGeneSet: {'HALLMARK_DNA_REPAIR'}, [11], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AADAT, 2, tensor([140000886135904, 140000886135904])\n",
       "\tGeneSet: {'HALLMARK_FATTY_ACID_METABOLISM'}, [16], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AARS, 3, tensor([                  0, 7017846510946824290, 3906419923676049764])\n",
       "\tGeneSet: {'HALLMARK_ALLOGRAFT_REJECTION'}, [1], tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABAT, 4, tensor([140000886135920,        74955568,              32,             128])\n",
       "\tGeneSet: {'HALLMARK_P53_PATHWAY', 'HALLMARK_ESTROGEN_RESPONSE_EARLY'}, [36, 14], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA1, 5, tensor([7236833184642769452, 7881144752318672485, 7310875413622977637,\n",
       "        7308895138223563308, 3472666057571973666])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_TNFA_SIGNALING_VIA_NFKB', 'HALLMARK_PROTEIN_SECRETION', 'HALLMARK_INFLAMMATORY_RESPONSE', 'HALLMARK_ADIPOGENESIS'}, [7, 44, 40, 24, 0], tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA2, 6, tensor([    140000886135952,            74779312, 4605671036451553280,\n",
       "                        113,            74934480,            74932640])\n",
       "\tGeneSet: {'HALLMARK_CHOLESTEROL_HOMEOSTASIS', 'HALLMARK_BILE_ACID_METABOLISM'}, [8, 7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA3, 7, tensor([    140000886135952,     140000886135952,                  32,\n",
       "                        112,            74779152, 7310593858020254331,\n",
       "        3472891254536807012])\n",
       "\tGeneSet: {'HALLMARK_ESTROGEN_RESPONSE_EARLY', 'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_ESTROGEN_RESPONSE_LATE'}, [14, 7, 15], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA4, 8, tensor([140000886135984,        75282720,               0,             113,\n",
       "               74775152,        74776336,              -1,               0])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA5, 9, tensor([139999720757936, 139999720850736, 139999720757992, 139999720758048,\n",
       "        139999720758104, 139999720758160, 139999720758216, 139999720758272,\n",
       "        139999720758328])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('data/msigdb/small')\n",
    "dataset = DatasetMsigDb(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene2GeneSetModule(nn.Module):\n",
    "    def __init__(self, dataset_msigdb, embedding_dim, layer_size=10):\n",
    "        super(Gene2GeneSetModule, self).__init__()\n",
    "        genes_vocab_size = len(dataset_msigdb)\n",
    "        genesets_vocab_size = dataset_msigdb.gene_sets_size()\n",
    "        self.embeddings = nn.Embedding(genes_vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, layer_size)\n",
    "        self.linear2 = nn.Linear(layer_size, genesets_vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.embeddings(inputs))\n",
    "        x = F.relu(self.linear1(x))\n",
    "        probs = torch.sigmoid(self.linear2(x))\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, lr, momentum):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for n_epoch in range(epochs):\n",
    "        for n_batch, batch in enumerate(dataloader):\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = F.binary_cross_entropy(output.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if n_epoch % 1 == 0 and n_batch == 0:\n",
    "                print(f\"Train Epoch: {n_epoch} / {epochs}\\tn_batch: {n_batch}\\tLoss: {loss.item():.6f}\\tx.shape: {x.shape}\\ty.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1000)\n",
    "model = Gene2GeneSetModule(dataset, 20, 10)\n",
    "train(model, dataloader, epochs=100, lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7577,  0.2743, -0.8322]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.LongTensor([[1]])\n",
    "emb(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgen",
   "language": "python",
   "name": "mlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
