{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene2Vec in Pytorch: Part 2\n",
    "\n",
    "\n",
    "\n",
    "This time we solve the exact same problem, but with a few improvements.\n",
    "\n",
    "In the previous version, we created one-hot input vectors.\n",
    "\n",
    "The input is then applied to the first linear layer in the network, since all but one of the inputs are 0 (one-hot encoded), only one of the rows of the layer will be used (i.e. the one corresponding to the '1' in the input). So this is wastefull because we are spending a lot of time multiplying by zeros. Pytorch provides an `Embedding` layer that speeds this up by performing a lookup (much faster). Also we use less memory since we now only have to provide an input index, instead of a one-hot encoded vertor of dimention 23,112.\n",
    "\n",
    "Changes in this version\n",
    "- Network: Use Pytorch Embedding, instead of linear layer\n",
    "- Dataset inputs: Use single number (gene index), instead of one-hot encoded vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tcga.msigdb import *\n",
    "from tcga.util import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names2num(names):\n",
    "    \"\"\" Create a mapping from names to numbers \"\"\"\n",
    "    names = list(set(names))  # Make sure names are unique\n",
    "    return {n: i for i,n in enumerate(sorted(names))}\n",
    "    \n",
    "class DatasetMsigDb(Dataset):\n",
    "    \"\"\" \n",
    "    Custom dataset: We have to override methods __len__ and __getitem__\n",
    "    In our network, the inputs are genes and the outputs are gene-sets.\n",
    "    We convert genes and gene sets to numbers, then store the forward and\n",
    "    reverse mappings.\n",
    "    \n",
    "    Genes: One hot encoding\n",
    "    \n",
    "    Gene sets: We encode gene-> gene_sets as a dictionary indexed by\n",
    "    gene, with tensors having 1 on the GeneSets the gene belongs to.\n",
    "    \n",
    "    The method __getitem__ returns a tuple with the gene (one-hot\n",
    "    encoded) and the gene-set tensor (having ones on all gene-sets\n",
    "    the gene belongs to)\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.msigdb = read_msigdb_all(path)\n",
    "        # Gene <-> number: forward and reverse mapping\n",
    "        self.gene2num = names2num(msigdb2genes(self.msigdb))\n",
    "        self.num2gene = {n: g for g, n in self.gene2num.items()}\n",
    "        # GeneSet <-> number: forward and reverse mapping\n",
    "        self.geneset2num = names2num(msigdb2gene_sets(self.msigdb))\n",
    "        self.num2geneset = {n: g for g, n in self.gene2num.items()}\n",
    "        # Gene -> GeneSets mapping (use gene_set numbers, in a tensor)\n",
    "        self.init_genes()\n",
    "        self.init_genesets()\n",
    "\n",
    "    def genesets2num(self, genesets):\n",
    "        \" Convert to a list of numerically encoded gene-sets \"\n",
    "        return [self.geneset2num[gs] for gs in genesets]\n",
    "\n",
    "    def gene2tensor(self, gene):\n",
    "        \" Convert to an index tensor (yes, it's just a number) \"\n",
    "        gene_tensor = torch.LongTensor([self.gene2num[gene]])\n",
    "        return gene_tensor\n",
    "        \n",
    "    def genesets2tensor(self, genesets):\n",
    "        \" Convert to a vector having 1 in each geneset position \"\n",
    "        geneset_idxs = [self.geneset2num[gs] for gs in genesets]\n",
    "        geneset_tensor = torch.zeros(len(self.msigdb))\n",
    "        geneset_tensor[geneset_idxs] = 1\n",
    "        return geneset_tensor\n",
    "        \n",
    "    def init_genes(self):\n",
    "        \" Create a one-hot encoding for a gene \"\n",
    "        self.gene_tensors = dict()\n",
    "        for gene in self.gene2num.keys():\n",
    "            self.gene_tensors[gene] = self.gene2tensor(gene)\n",
    "        \n",
    "    def init_genesets(self):\n",
    "        \" Map Gene to GeneSets. GeneSets are hot-encoded \"\n",
    "        self.gene_genesets = dict()\n",
    "        self.gene_genesets_num = dict()\n",
    "        self.gene_genesets_tensors = dict()\n",
    "        num_genesets = len(self.geneset2num)\n",
    "        for gene, genesets in gene_genesets(self.msigdb).items():\n",
    "            self.gene_genesets[gene] = genesets\n",
    "            self.gene_genesets_num[gene] = self.genesets2num(genesets)\n",
    "            self.gene_genesets_tensors[gene] = self.genesets2tensor(genesets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \" Len: Count number of genes \"\n",
    "        return len(self.gene2num)\n",
    "\n",
    "    def gene_sets_size(self):\n",
    "        \" Count number of gene sets \"\n",
    "        return len(self.msigdb)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \" Get item 'idx': A tuple of gene number 'idx' and gene set tensor for that gene \"\n",
    "        gene = self.num2gene[idx]\n",
    "        return (self.gene_tensors[gene], self.gene_genesets_tensors[gene])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        \" Show (a few) mappings gene -> gene_set tensor \"\n",
    "        out = f\"Genes: {len(self.gene2num)}, Gene Sets: {len(self.geneset2num)}\\n\"\n",
    "        for i in range(10):  #range(len(self)):\n",
    "            gene = self.num2gene[i]\n",
    "            gene_tensor, geneset_tensor = self[i]\n",
    "            out += f\"\\tGene: {gene}, {i}, {gene_tensor}\\n\\tGeneSet: {self.gene_genesets[gene]}, {self.gene_genesets_num[gene]}, {geneset_tensor}\\n\\n\"\n",
    "        return out + \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/msigdb/small/h.all.v7.0.symbols.gmt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Genes: 4384, Gene Sets: 50\n",
       "\tGene: A2M, 0, tensor([0])\n",
       "\tGeneSet: {'HALLMARK_COAGULATION', 'HALLMARK_IL6_JAK_STAT3_SIGNALING'}, [9, 23], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AAAS, 1, tensor([1])\n",
       "\tGeneSet: {'HALLMARK_DNA_REPAIR'}, [11], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AADAT, 2, tensor([2])\n",
       "\tGeneSet: {'HALLMARK_FATTY_ACID_METABOLISM'}, [16], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AARS, 3, tensor([3])\n",
       "\tGeneSet: {'HALLMARK_ALLOGRAFT_REJECTION'}, [1], tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABAT, 4, tensor([4])\n",
       "\tGeneSet: {'HALLMARK_ESTROGEN_RESPONSE_EARLY', 'HALLMARK_P53_PATHWAY'}, [14, 36], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA1, 5, tensor([5])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_PROTEIN_SECRETION', 'HALLMARK_INFLAMMATORY_RESPONSE', 'HALLMARK_TNFA_SIGNALING_VIA_NFKB', 'HALLMARK_ADIPOGENESIS'}, [7, 40, 24, 44, 0], tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA2, 6, tensor([6])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_CHOLESTEROL_HOMEOSTASIS'}, [7, 8], tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA3, 7, tensor([7])\n",
       "\tGeneSet: {'HALLMARK_ESTROGEN_RESPONSE_EARLY', 'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_ESTROGEN_RESPONSE_LATE'}, [14, 7, 15], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA4, 8, tensor([8])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA5, 9, tensor([9])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('data/msigdb/small')\n",
    "dataset = DatasetMsigDb(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene2GeneSetModule(nn.Module):\n",
    "    def __init__(self, dataset_msigdb, embedding_dim, layer_size=10):\n",
    "        super(Gene2GeneSetModule, self).__init__()\n",
    "        genes_vocab_size = len(dataset_msigdb)\n",
    "        genesets_vocab_size = dataset_msigdb.gene_sets_size()\n",
    "        self.embeddings = nn.Embedding(genes_vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, layer_size)\n",
    "        self.linear2 = nn.Linear(layer_size, genesets_vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.embeddings(inputs))\n",
    "        x = F.relu(self.linear1(x))\n",
    "        probs = torch.sigmoid(self.linear2(x))\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, lr, momentum):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for n_epoch in range(epochs):\n",
    "        for n_batch, batch in enumerate(dataloader):\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = F.binary_cross_entropy(output.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if n_epoch % 10 == 0 and n_batch == 0:\n",
    "                print(f\"Train Epoch: {n_epoch} / {epochs}\\tn_batch: {n_batch}\\tLoss: {loss.item():.6f}\\tx.shape: {x.shape}\\ty.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 / 150\tn_batch: 0\tLoss: 0.690097\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 10 / 150\tn_batch: 0\tLoss: 0.149002\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 20 / 150\tn_batch: 0\tLoss: 0.140674\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 30 / 150\tn_batch: 0\tLoss: 0.137686\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 40 / 150\tn_batch: 0\tLoss: 0.135548\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 50 / 150\tn_batch: 0\tLoss: 0.132318\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 60 / 150\tn_batch: 0\tLoss: 0.125862\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 70 / 150\tn_batch: 0\tLoss: 0.115653\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 80 / 150\tn_batch: 0\tLoss: 0.103543\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 90 / 150\tn_batch: 0\tLoss: 0.091850\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 100 / 150\tn_batch: 0\tLoss: 0.082020\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 110 / 150\tn_batch: 0\tLoss: 0.074528\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 120 / 150\tn_batch: 0\tLoss: 0.068965\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 130 / 150\tn_batch: 0\tLoss: 0.064826\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 140 / 150\tn_batch: 0\tLoss: 0.061566\tx.shape: torch.Size([1000, 1])\ty.shape: torch.Size([1000, 50])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1000)\n",
    "model = Gene2GeneSetModule(dataset, 20, 10)\n",
    "train(model, dataloader, epochs=150, lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgen",
   "language": "python",
   "name": "mlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
