{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-values in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from scipy.stats import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(h):\n",
    "    ''' Logistic from activation h '''\n",
    "    p = 1.0 / (1.0 + np.exp(-h))\n",
    "    r = np.random.rand(len(p))\n",
    "    y = (r < p).astype('float')\n",
    "    return y\n",
    "\n",
    "\n",
    "def rand_date():\n",
    "    max_time = int(time.time())\n",
    "    t = random.randint(0, max_time)\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "\n",
    "\n",
    "def rand_unif(num, mean, std, na_prob=0):\n",
    "    xi = np.random.rand(num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def rand_norm(num, mean=0.0, std=1.0, na_prob=0):\n",
    "    xi = np.random.normal(mean, std, num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def create_dataset_01(num=1000, n_rands=5, save=False):\n",
    "    \"\"\" Create dataset y = f(x1, x2, x3) + noise (r* are not used) \"\"\"\n",
    "    x1 = rand_norm(num)\n",
    "    x2 = rand_norm(num)\n",
    "    x3 = rand_norm(num)\n",
    "    n = rand_norm(num)\n",
    "    y = logit(3.0 * x1 - 2.0 * x2 + 1.0 * x3 + 0.5 * n)\n",
    "    d = {'x1': x1, 'x2': x2, 'x3': x3, 'y': y}\n",
    "    for i in range(n_rands):\n",
    "        d[f\"rand_{i}\"] = rand_norm(num)\n",
    "    df = pd.DataFrame(d)\n",
    "    if save:\n",
    "        file = 'zzz.csv'\n",
    "        print(f\"Saving dataset to file '{file}'\")\n",
    "        df.to_csv(file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-values and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilks_model_fit(x, y):\n",
    "    logit_model = Logit(y, x)\n",
    "    res = logit_model.fit(disp=0)\n",
    "    return logit_model, res\n",
    "\n",
    "def wilks_p_value(df, var_output, vars_null):\n",
    "    model_null, model_null_results = wilks_model_fit(df[vars_null], df[var_output])\n",
    "    pvalues = dict()\n",
    "    for c in df.columns:\n",
    "        if c != var_output and c not in vars_null:\n",
    "            xnames = list(vars_null)\n",
    "            xnames.append(c)\n",
    "            model_alt, model_alt_res = wilks_model_fit(df[xnames], df[var_output])\n",
    "            if model_alt is None:\n",
    "                self._error(f\"Could not fit alt model for column/s {c}\")\n",
    "                pval = 1.0\n",
    "            else:\n",
    "                d = 2.0 * (model_alt_res.llf - model_null_results.llf)\n",
    "                pval = chi2.sf(d, 1)\n",
    "            pvalues[c] = pval\n",
    "    return pd.Series(pvalues)\n",
    "\n",
    "def feature_importance(model, x, y, var):\n",
    "    score_null = model.score(x, y)\n",
    "    x_shuf = x.copy()\n",
    "    x_shuf[var] = x_shuf[var].sample(frac=1).values\n",
    "    score_alt = model.score(x_shuf, y)\n",
    "    return (score_null - score_alt) / score_null\n",
    "\n",
    "def feature_importance_multiple_shuffle(model, x_validate, y_validate, vars_input, num_iter=10, scores=None):\n",
    "    scores = dict() if scores is None else scores\n",
    "    for var in vars_input:\n",
    "        delta_scores = list()\n",
    "        for i in range(num_iter):\n",
    "            delta_score = feature_importance(model, x_validate, y_validate, var)\n",
    "            delta_scores.append(delta_score)\n",
    "        delta_scores = np.array(delta_scores)\n",
    "        # print(f\"{var}:\\tmean: {delta_scores.mean()}\\tstd: {delta_scores.std()}\\tdelta_scores: {delta_scores}\")\n",
    "        # Append values to array (if any)\n",
    "        scores[var] = delta_scores if var not in scores else np.append(scores[var], delta_scores).flatten()\n",
    "    return scores\n",
    "\n",
    "def split_df(df, var_inputs, var_output, train_index, val_index):\n",
    "    df_train, df_val = df.iloc[train_index], df.iloc[val_index]\n",
    "    return df_train[var_inputs], df_train[var_output], df_val[var_inputs], df_val[var_output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One model, one data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Function to create new model\n",
    "def new_model(): return RandomForestClassifier(100)\n",
    "\n",
    "# Create dataset\n",
    "def p_values_shuffle(num_samples=100, n_rands=5, num_cv=1, num_shuffle=10, show_plot=False, pvals_df=None):\n",
    "    df = create_dataset_01(num=num_samples, n_rands=n_rands)\n",
    "    if show_plot:\n",
    "        sns.pairplot(df, kind='scatter', diag_kind='kde')\n",
    "    # Variables\n",
    "    var_output = 'y'\n",
    "    vars_input = [c for c in df.columns if c != var_output]\n",
    "    vars_null = [c for c in vars_input if c.startswith('rand_')]\n",
    "    # p-values from Wilks model\n",
    "    #pval_wilks = wilks_p_value(df, var_output, vars_null)\n",
    "    # Create several models...\n",
    "    scores = dict()\n",
    "    if num_cv > 1:\n",
    "        cv_iter = KFold(n_splits=num_cv).split(df)\n",
    "    else:\n",
    "        # No cross validation, split 80% / 20%\n",
    "        idx = int(0.8 * len(df))\n",
    "        idx_train = range(0, idx)\n",
    "        idx_val = range(idx, len(df))\n",
    "        cv_iter = [(idx_train, idx_val)]\n",
    "    cv_count = 0\n",
    "    for train_index, val_index in cv_iter:\n",
    "        cv_count += 1\n",
    "        # print(f\"Cross validation: {cv_count} / {num_cv}\")\n",
    "        # Split dataset\n",
    "        x_train, y_train, x_validate, y_validate = split_df(df, vars_input, var_output, train_index, val_index)\n",
    "        # Create model\n",
    "        model = new_model()\n",
    "        model_fit = model.fit(x_train, y_train)\n",
    "        # Calculate scores (shuffle)\n",
    "        scores = feature_importance_multiple_shuffle(model, x_validate, y_validate, vars_input, num_iter=num_shuffle, scores=scores)\n",
    "    # Calculate p-values\n",
    "    pvals_df = pd.DataFrame() if pvals_df is None else pvals_df\n",
    "    null_scores = np.array([scores[c] for c in vars_null]).flatten()\n",
    "    for c in vars_input:\n",
    "        null_scores = np.array([scores[cn] for cn in vars_null if cn != c]).flatten()\n",
    "        pval = sc.stats.mannwhitneyu(scores[c], null_scores, alternative='greater')[1]\n",
    "        df_row = pd.DataFrame({'name': c,\n",
    "            'num_samples': num_samples, 'n_rands': n_rands,\n",
    "            'num_cv': num_cv, 'num_shuffle': num_shuffle,\n",
    "            'count_alt': len(scores[c])\n",
    "                      , 'mean_alt': scores[c].mean(), 'std_alt': scores[c].std()\n",
    "                      , 'count_null': len(null_scores), 'mean_null': null_scores.mean()\n",
    "                      , 'std_null': null_scores.std()\n",
    "                      , 'p_value': pval}, index=[len(pvals_df)])\n",
    "        pvals_df = pvals_df.append(df_row)\n",
    "    return pvals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pvals_df = None\n",
    "# for num_cv in [1, 3, 5, 10, 20]:\n",
    "#     for num_samples in [50, 100, 200, 300, 400, 500, 1000, 2000, 10000]:\n",
    "#         for num_shuffle in [3, 5, 10, 20, 50, 100]:\n",
    "#             pvals_df = p_values_shuffle(num_samples=num_samples, n_rands=5, num_shuffle=num_shuffle, pvals_df=pvals_df)\n",
    "#             print(f\"num_cv: {num_cv}\\tnum_samples:{num_samples}\\tnum_shuffle:{num_shuffle}\\t{pvs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cv: 1\tnum_samples:50\tnum_shuffle:3\n",
      "num_cv: 1\tnum_samples:50\tnum_shuffle:5\n",
      "num_cv: 1\tnum_samples:100\tnum_shuffle:3\n",
      "num_cv: 1\tnum_samples:100\tnum_shuffle:5\n",
      "num_cv: 2\tnum_samples:50\tnum_shuffle:3\n"
     ]
    }
   ],
   "source": [
    "pvals_df = None\n",
    "for num_cv in [1, 2]:\n",
    "    for num_samples in [50, 100]:\n",
    "        for num_shuffle in [3, 5]:\n",
    "            pvals_df = p_values_shuffle(num_samples=num_samples, n_rands=5, num_cv=num_cv, num_shuffle=num_shuffle, pvals_df=pvals_df)\n",
    "            print(f\"num_cv: {num_cv}\\tnum_samples:{num_samples}\\tnum_shuffle:{num_shuffle}\")\n",
    "pvals_df.to_csv('pvalues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_df.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logml",
   "language": "python",
   "name": "logml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
