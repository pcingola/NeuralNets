{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-values in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from scipy.stats import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(h):\n",
    "    ''' Logistic from activation h '''\n",
    "    p = 1.0 / (1.0 + np.exp(-h))\n",
    "    r = np.random.rand(len(p))\n",
    "    y = (r < p).astype('float')\n",
    "    return y\n",
    "\n",
    "\n",
    "def rand_date():\n",
    "    max_time = int(time.time())\n",
    "    t = random.randint(0, max_time)\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "\n",
    "\n",
    "def rand_unif(num, mean, std, na_prob=0):\n",
    "    xi = np.random.rand(num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def rand_norm(num, mean=0.0, std=1.0, na_prob=0):\n",
    "    xi = np.random.normal(mean, std, num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def create_dataset_01(num=1000, n_rands=5, save=False):\n",
    "    \"\"\" Create dataset y = f(x1, x2, x3) + noise (r* are not used) \"\"\"\n",
    "    x1 = rand_norm(num)\n",
    "    x2 = rand_norm(num)\n",
    "    x3 = rand_norm(num)\n",
    "    n = rand_norm(num)\n",
    "    y = logit(2.0 * x1 - 1.0 * x2 + 0.5 * x3 + 0.1 * n)\n",
    "    d = {'x1': x1, 'x2': x2, 'x3': x3, 'y': y}\n",
    "    for i in range(n_rands):\n",
    "        d[f\"rand_{i}\"] = rand_norm(num)\n",
    "    df = pd.DataFrame(d)\n",
    "    if save:\n",
    "        file = 'zzz.csv'\n",
    "        print(f\"Saving dataset to file '{file}'\")\n",
    "        df.to_csv(file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-values and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilks_model_fit(x, y):\n",
    "    logit_model = Logit(y, x)\n",
    "    res = logit_model.fit(disp=0)\n",
    "    return logit_model, res\n",
    "\n",
    "def wilks_p_value(df, var_output, vars_null):\n",
    "    model_null, model_null_results = wilks_model_fit(df[vars_null], df[var_output])\n",
    "    pvalues = dict()\n",
    "    for c in df.columns:\n",
    "        if c != var_output and c not in vars_null:\n",
    "            xnames = list(vars_null)\n",
    "            xnames.append(c)\n",
    "            model_alt, model_alt_res = wilks_model_fit(df[xnames], df[var_output])\n",
    "            if model_alt is None:\n",
    "                self._error(f\"Could not fit alt model for column/s {c}\")\n",
    "                pval = 1.0\n",
    "            else:\n",
    "                d = 2.0 * (model_alt_res.llf - model_null_results.llf)\n",
    "                pval = chi2.sf(d, 1)\n",
    "            pvalues[c] = pval\n",
    "    return pd.Series(pvalues)\n",
    "\n",
    "def feature_importance(model, x, y, var):\n",
    "    score_null = model.score(x, y)\n",
    "    x_shuf = x.copy()\n",
    "    x_shuf[var] = x_shuf[var].sample(frac=1).values\n",
    "    score_alt = model.score(x_shuf, y)\n",
    "    return (score_null - score_alt) / score_null\n",
    "\n",
    "def feature_importance_multiple_shuffle(model, x_validate, y_validate, vars_input, num_iter=10, scores=None):\n",
    "    scores = dict() if scores is None else scores\n",
    "    for var in vars_input:\n",
    "        delta_scores = list()\n",
    "        for i in range(num_iter):\n",
    "            delta_score = feature_importance(model, x_validate, y_validate, var)\n",
    "            delta_scores.append(delta_score)\n",
    "        delta_scores = np.array(delta_scores)\n",
    "        # print(f\"{var}:\\t{delta_scores.mean()}\\t{delta_scores.std()}\")\n",
    "        scores[var] = delta_scores\n",
    "    return scores\n",
    "\n",
    "def split_df(df, val_perc=0.2):\n",
    "    idx = int(len(df) * (1.0 -val_perc))\n",
    "    return df[:idx], df[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One model, one data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create dataset\n",
    "def p_values_shuffle(num_samples=100, n_rands=5, num_shuffle=10):\n",
    "    df = create_dataset_01(num=num_samples, n_rands=n_rands)\n",
    "    # Variables\n",
    "    var_output = 'y'\n",
    "    vars_input = [c for c in df.columns if c != var_output]\n",
    "    vars_null = [c for c in vars_input if c.startswith('rand_')]\n",
    "    # p-values from Wilks model\n",
    "    pval_wilks = wilks_p_value(df, var_output, vars_null)\n",
    "    # Split dataset\n",
    "    df_train, df_validate = split_df(df, val_perc=0.2)\n",
    "    x_train, y_train = df_train[vars_input], df_train[var_output]\n",
    "    x_validate, y_validate = df_validate[vars_input], df_validate[var_output]\n",
    "    x_train.shape, y_train.shape, x_validate.shape, y_validate.shape\n",
    "    # Create model\n",
    "    model = RandomForestClassifier(100)\n",
    "    model_fit = model.fit(x_train, y_train)\n",
    "    # Calculate scores (shuffle)\n",
    "    scores = feature_importance_multiple_shuffle(model, x_validate, y_validate, vars_input, num_iter=num_shuffle)\n",
    "    # Calculate p-values\n",
    "    null_scores = np.array([scores[c] for c in vars_null]).flatten()\n",
    "    pvals = {c: sc.stats.mannwhitneyu(scores[c], null_scores, alternative='greater')[1] for c in vars_input if c not in vars_null}\n",
    "    return pvals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_samples in [50, 100, 200, 300, 400, 500, 1000, 2000, 10000]:\n",
    "    for num_shuffle in [3, 5, 10, 20, 50, 100]:\n",
    "        pvs = p_values_shuffle(num_samples=num_samples, n_rands=5, num_shuffle=num_shuffle)\n",
    "        print(f\"num_samples:{num_samples}\\tnum_shuffle:{num_shuffle}\\t{pvs}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logml",
   "language": "python",
   "name": "logml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
