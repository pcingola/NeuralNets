{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-values in ML 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from scipy.stats import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(h):\n",
    "    ''' Logistic from activation h '''\n",
    "    p = 1.0 / (1.0 + np.exp(-h))\n",
    "    r = np.random.rand(len(p))\n",
    "    y = (r < p).astype('float')\n",
    "    return y\n",
    "\n",
    "\n",
    "def rand_date():\n",
    "    max_time = int(time.time())\n",
    "    t = random.randint(0, max_time)\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "\n",
    "\n",
    "def rand_unif(num, mean, std, na_prob=0):\n",
    "    xi = np.random.rand(num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def rand_norm(num, mean=0.0, std=1.0, na_prob=0):\n",
    "    xi = np.random.normal(mean, std, num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def rbf(x, mu=0, sigma=1):\n",
    "    \"\"\" Radial basis function \"\"\"\n",
    "    z = (x - mu) / sigma\n",
    "    return np.exp(-(z*z))\n",
    "\n",
    "\n",
    "def split_df(df, var_inputs, var_output, train_index, val_index):\n",
    "    df_train, df_val = df.iloc[train_index], df.iloc[val_index]\n",
    "    return df_train[var_inputs], df_train[var_output], df_val[var_inputs], df_val[var_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_01(num=1000, save=False):\n",
    "    \"\"\" Create dataset y = f(x1, x2, x3) + noise (r* are not used) \"\"\"\n",
    "    # Inputs\n",
    "    x1 = rand_norm(num)\n",
    "    x2 = rand_norm(num)\n",
    "    x3 = rand_norm(num)\n",
    "    # Noise\n",
    "    n1 = rand_norm(num)\n",
    "    n2 = rand_norm(num)\n",
    "    n3 = rand_norm(num)\n",
    "    n = rand_norm(num)\n",
    "    y = logit(2.0 * x1 - 1.0 * x2 + 0.5 * x3 + 0.25 * n)\n",
    "    df = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'n1': n1, 'n2': n2, 'n3': n3, 'y': y})\n",
    "    if save:\n",
    "        file = 'zzz.csv'\n",
    "        print(f\"Saving dataset to file '{file}'\")\n",
    "        df.to_csv(file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_02(num=1000, save=False):\n",
    "    \"\"\" Create dataset y = f(x1, x2, x3) + noise (r* are not used) \"\"\"\n",
    "    # Inputs\n",
    "    x1 = rand_norm(num)\n",
    "    x2 = rand_norm(num)\n",
    "    x3 = rand_norm(num)\n",
    "    # Noise\n",
    "    n1 = rand_norm(num)\n",
    "    n2 = rand_norm(num)\n",
    "    n3 = rand_norm(num)\n",
    "    n = rand_norm(num)\n",
    "    y = logit(2.0 * rbf(x1, 1, 1) - 1.0 * rbf(x2, -1, 1) + 0.5 * x3 + 0.25 * n)\n",
    "    df = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'n1': n1, 'n2': n2, 'n3': n3, 'y': y})\n",
    "    if save:\n",
    "        file = 'zzz.csv'\n",
    "        print(f\"Saving dataset to file '{file}'\")\n",
    "        df.to_csv(file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilks_model_fit(x, y):\n",
    "    logit_model = Logit(y, x)\n",
    "    res = logit_model.fit(disp=0)\n",
    "    return logit_model, res\n",
    "\n",
    "def wilks_p_value(df, var_output, vars_null):\n",
    "    model_null, model_null_results = wilks_model_fit(df[vars_null], df[var_output])\n",
    "    pvalues = dict()\n",
    "    for c in df.columns:\n",
    "        if c != var_output and c not in vars_null:\n",
    "            xnames = list(vars_null)\n",
    "            xnames.append(c)\n",
    "            model_alt, model_alt_res = wilks_model_fit(df[xnames], df[var_output])\n",
    "            if model_alt is None:\n",
    "                self._error(f\"Could not fit alt model for column/s {c}\")\n",
    "                pval = 1.0\n",
    "            else:\n",
    "                d = 2.0 * (model_alt_res.llf - model_null_results.llf)\n",
    "                pval = chi2.sf(d, 1)\n",
    "            pvalues[c] = pval\n",
    "    return pd.Series(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance: ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validations & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def new_model_fit(x_train, y_train):\n",
    "    model = RandomForestClassifier(100)\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "_id = 0\n",
    "\n",
    "def reset_id():\n",
    "    _id = 0\n",
    "\n",
    "def next_id():\n",
    "    global _id\n",
    "    _id += 1\n",
    "    return _id\n",
    "\n",
    "\n",
    "def is_var_rand(var):\n",
    "    return var is not None and var.startswith('__rand_')\n",
    "\n",
    "\n",
    "def p_value_mannwhitneyu(null_scores, alt_scores, alternative):\n",
    "    try:\n",
    "        return sc.stats.mannwhitneyu(null_scores, alt_scores, alternative=alternative)[1]\n",
    "    except ValueError as ve:\n",
    "        print(f\"WARNING: Mann-Whitney U-test, {ve}\")\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "class FeatureImportanceShuffle:\n",
    "    \"\"\" Feature importance by shuffling input values \"\"\"\n",
    "    def __init__(self, model, x_train, y_train, x_val, y_val, num_iter=10):\n",
    "        self.id = next_id()\n",
    "        self.model = model\n",
    "        self.x_train, self.y_train, self.x_val, self.y_val = x_train, y_train, x_val, y_val\n",
    "        self.num_iter = num_iter\n",
    "        self.score_null = None  # Null score (one score)\n",
    "        self.scores_alt = dict()  # Alt scores by variable name (multiple scores for each variable)\n",
    "\n",
    "    def add_score(self, var, score):\n",
    "        \"\"\" Add score for variable 'var' \"\"\"\n",
    "        if var not in self.scores_alt:\n",
    "            self.scores_alt[var] = list()\n",
    "        self.scores_alt[var].append(score)\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\" Feature importance by shuffling variable 'var' and comparing performance results \"\"\"\n",
    "        if not self.score_null:\n",
    "            self.score_null = self.model.score(self.x_val, self.y_val)\n",
    "            # print(f\"\\t\\tFeature importance (id={self.id}): Null model {self.scores_null}\")\n",
    "        for var in self.x_train.columns:\n",
    "            # Perform shuffling 'num_iter' times\n",
    "            scores = list()\n",
    "            for i in range(self.num_iter):\n",
    "                score = self.feature_importance_variable(var)\n",
    "                scores.append(score)\n",
    "                self.add_score(var, score)\n",
    "            scores = np.array(scores)\n",
    "            # print(f\"\\t{var}:\\tcount: {len(scores)}\\tmean: {scores.mean()}\\tstd: {scores.std()}\")\n",
    "\n",
    "    def feature_importance_variable(self, var):\n",
    "        \"\"\" Feature importance by shuffling variable 'var' and comparing performance results \"\"\"\n",
    "        # Shuffle 'var' and calculate validation score\n",
    "        x_shuf = self.x_val.copy()\n",
    "        x_shuf[var] = x_shuf[var].sample(frac=1).values\n",
    "        score = self.model.score(x_shuf, self.y_val)\n",
    "        # print(f\"\\t\\tFeature importance (id={self.id}): Variable '{var}, score: {score}'\")\n",
    "        return score\n",
    "\n",
    "    def get_null(self):\n",
    "        \"\"\" Get null score \"\"\"\n",
    "        return self.score_null\n",
    "\n",
    "    def get_null_and_rand(self, exclude_var=None):\n",
    "        \"\"\" Get null and all '__rand_' scores as a list \"\"\"\n",
    "        values = [self.score_null]\n",
    "        for var in self.x_train.columns:\n",
    "            if is_var_rand(var) and var != exclude_var:\n",
    "                values.extend(self.scores_alt[var])\n",
    "        return values\n",
    "\n",
    "    def get_alt(self, var):\n",
    "        \"\"\" Get scores for variable 'var', or None if not found \"\"\"\n",
    "        return self.scores_alt.get(var)\n",
    "\n",
    "\n",
    "class FeatureImportance:\n",
    "    \"\"\"\n",
    "    Calculate feature importance.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, var_output, vars_input=None, vars_null=None, new_model_function=new_model_fit, num_cv=5, num_shuffle=10, num_rand=0, max_iter=100):\n",
    "        self.df_ori = df\n",
    "        self.var_output, self.vars_input, self.vars_null = var_output, vars_input, vars_null\n",
    "        if self.vars_input is None:\n",
    "            self.vars_input = [c for c in df.columns if c != var_output]\n",
    "        if self.vars_null is None:\n",
    "            self.vars_null = [c for c in self.vars_input if c.startswith('rand_')]\n",
    "        self.new_model_function = new_model_function\n",
    "        self.df = None\n",
    "        self.pvals_df = None\n",
    "        self.max_iter =max_iter\n",
    "        self.num_cv = num_cv\n",
    "        self.num_rand = num_rand\n",
    "        self.num_shuffle = num_shuffle\n",
    "        self.cvs = list()  # List of cross validation items\n",
    "    \n",
    "    def __call__(self):\n",
    "        # initialize\n",
    "        self.df = self.extend_df_ori()\n",
    "        self.initialize_cv()\n",
    "        # Calculate importance by shuffling\n",
    "        for count_iter in range(self.max_iter):\n",
    "            self.pvals_df = pd.DataFrame()\n",
    "            for fis in self.cvs:\n",
    "                print(f\"Iteration: {count_iter}, Cross-validation: {fis.id}\")\n",
    "                fis()\n",
    "            stop = self.p_values()\n",
    "            if stop:\n",
    "                return self.pvals_df\n",
    "        return self.pvals_df\n",
    "    \n",
    "    def cv_iter(self, df):\n",
    "        \"\"\" Create a cross-validation iterator \"\"\"\n",
    "        if self.num_cv > 1:\n",
    "            return KFold(n_splits=self.num_cv).split(df)\n",
    "        else:\n",
    "            # No cross validation, split 80% / 20%\n",
    "            idx = int(0.8 * len(df))\n",
    "            idx_train = range(0, idx)\n",
    "            idx_val = range(idx, len(df))\n",
    "            return [(idx_train, idx_val)]\n",
    "        \n",
    "    def extend_df_ori(self):\n",
    "        \"\"\" Extend the data frame by adding random permutations and random variables \"\"\"\n",
    "        df = self.df_ori.copy()\n",
    "        # Create one additional 'rand' column for every variable\n",
    "        for c in self.df_ori.columns:\n",
    "            if c == self.var_output:\n",
    "                continue\n",
    "            c_rand = f\"__rand_{c}\"\n",
    "            self.vars_input.append(c_rand)\n",
    "            df[c_rand] = df[c].sample(frac=1).values\n",
    "        # Create one additional 'rand' column for every variable\n",
    "        for i in range(self.num_rand):\n",
    "            c_rand = f\"__rand_{i}\"\n",
    "            self.vars_input.append(c_rand)\n",
    "            df[c_rand] = np.random.rand(len(df[c]))\n",
    "        return df\n",
    "        \n",
    "    def get_alts(self, var):\n",
    "        \"\"\" Get alt scores for variable 'var' \"\"\"\n",
    "        scores = list()\n",
    "        for fis in self.cvs:\n",
    "            ss = fis.get_alt(var)\n",
    "            if ss is not None:\n",
    "                scores.extend(ss)\n",
    "        return np.array(scores)\n",
    "\n",
    "    def get_nulls(self, var):\n",
    "        \"\"\" Get null scores, excluding variable 'var' \"\"\"\n",
    "        scores = list()\n",
    "        for fis in self.cvs:\n",
    "            ss = fis.get_null_and_rand(exclude_var=var)\n",
    "            if ss is not None:\n",
    "                scores.extend(ss)\n",
    "        return np.array(scores)\n",
    "\n",
    "    def initialize_cv(self):\n",
    "        \"\"\" Initialize cross validations \"\"\"\n",
    "        reset_id()\n",
    "        for train_index, val_index in self.cv_iter(self.df):\n",
    "            x_train, y_train, x_validate, y_validate = split_df(self.df, self.vars_input, self.var_output, train_index, val_index)\n",
    "            model = self.new_model_function(x_train, y_train)\n",
    "            fis = FeatureImportanceShuffle(model, x_train, y_train, x_validate, y_validate, num_iter=self.num_shuffle)\n",
    "            self.cvs.append(fis)\n",
    "\n",
    "    def p_value(self, var, null_scores, alt_scores):\n",
    "        null_scores, alt_scores = np.array(null_scores), np.array(alt_scores)\n",
    "        pval_greater = p_value_mannwhitneyu(null_scores, alt_scores, alternative='greater')\n",
    "        pval_less = p_value_mannwhitneyu(null_scores, alt_scores, alternative='less')\n",
    "        pval_two = p_value_mannwhitneyu(null_scores, alt_scores, alternative='two-sided')\n",
    "        df_row = pd.DataFrame({'name': var\n",
    "                               , 'num_samples': len(self.df_ori)\n",
    "                               , 'num_cv': self.num_cv\n",
    "                               , 'num_shuffle': self.num_shuffle\n",
    "                               , 'count_alt': len(alt_scores)\n",
    "                               , 'mean_alt': alt_scores.mean()\n",
    "                               , 'std_alt': alt_scores.std()\n",
    "                               , 'count_null': len(null_scores)\n",
    "                               , 'mean_null': null_scores.mean()\n",
    "                               , 'std_null': null_scores.std()\n",
    "                               , 'p_value': pval_greater\n",
    "                               , 'p_value_less': pval_less\n",
    "                               , 'pval_two_sided': pval_two\n",
    "                              }, index=[len(self.pvals_df)])\n",
    "        self.pvals_df = self.pvals_df.append(df_row)\n",
    "        return pval_greater\n",
    "\n",
    "    def p_values(self):\n",
    "        # Get null scores\n",
    "        null_scores = list()\n",
    "        for fis in self.cvs:\n",
    "            null_scores.extend(fis.get_null_and_rand())\n",
    "        # Calculate p-value for every variable\n",
    "        pvals, isrand = list(), list()\n",
    "        for var in self.vars_input:\n",
    "            alt_scores = self.get_alts(var)\n",
    "            null_scores = self.get_nulls(var)\n",
    "            pval = self.p_value(var, null_scores, alt_scores)\n",
    "            pvals.append(pval)\n",
    "            isrand.append(is_var_rand(var))\n",
    "        pvals = np.array(pvals)\n",
    "        isrand = np.array(isrand)\n",
    "        rejected, pvals_fdr = fdrcorrection(pvals)\n",
    "        self.pvals_df['is_rand'] = isrand\n",
    "        self.pvals_df['pvalues_fdr'] = pvals_fdr\n",
    "        self.pvals_df['pvalues_fdr_reject'] = rejected\n",
    "        return (rejected & isrand).any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pvalues_shuffle_analysis():\n",
    "    pvals_df = None\n",
    "    for dataset_num in [1, 2]:\n",
    "        for num_samples in [50, 100, 200, 300, 500, 1000, 2000, 5000, 10000]:\n",
    "            for num_cv in [1, 3, 5, 10, 20]:\n",
    "                for num_shuffle in [3, 5, 10, 20, 50, 100]:\n",
    "                    print(f\"dataset_num: {dataset_num}\\tnum_samples: {num_samples}\\tnum_cv: {num_cv}\\tnum_shuffle: {num_shuffle}\")\n",
    "                    df = create_dataset_01(num_samples) if dataset_num == 1 else create_dataset_02(num_samples)\n",
    "                    fi = FeatureImportance(df, 'y', num_shuffle=num_shuffle, num_cv=num_cv)\n",
    "                    pvals_df = fi()\n",
    "                    pvals_df.to_csv(f\"pvals_df.dataset_num_{dataset_num}.num_samples_{num_samples}.num_cv_{num_cv}.num_shuffle_{num_shuffle}.csv\", index=False)\n",
    "                    display(pvals_df)\n",
    "\n",
    "                \n",
    "pvalues_shuffle_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement: Cluster high correlations (groups of variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgen",
   "language": "python",
   "name": "mlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
