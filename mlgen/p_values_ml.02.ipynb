{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-values in ML 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import scipy as sc\n",
    "import sklearn as sk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from scipy.stats import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(h):\n",
    "    ''' Logistic from activation h '''\n",
    "    p = 1.0 / (1.0 + np.exp(-h))\n",
    "    r = np.random.rand(len(p))\n",
    "    y = (r < p).astype('float')\n",
    "    return y\n",
    "\n",
    "\n",
    "def rand_date():\n",
    "    max_time = int(time.time())\n",
    "    t = random.randint(0, max_time)\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(t))\n",
    "\n",
    "\n",
    "def rand_unif(num, mean, std, na_prob=0):\n",
    "    xi = np.random.rand(num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def rand_norm(num, mean=0.0, std=1.0, na_prob=0):\n",
    "    xi = np.random.normal(mean, std, num)\n",
    "    if na_prob > 0:\n",
    "        xi_na = (np.random.rand(num) <= na_prob)\n",
    "        xi[xi_na] = np.nan\n",
    "    return xi\n",
    "\n",
    "\n",
    "def rbf(x, mu=0, sigma=1):\n",
    "    \"\"\" Radial basis function \"\"\"\n",
    "    z = (x - mu) / sigma\n",
    "    return np.exp(-(z*z))\n",
    "\n",
    "\n",
    "def split_df(df, var_inputs, var_output, train_index, val_index):\n",
    "    df_train, df_val = df.iloc[train_index], df.iloc[val_index]\n",
    "    return df_train[var_inputs], df_train[var_output], df_val[var_inputs], df_val[var_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_01(num=1000, n_rands=5, save=False):\n",
    "    \"\"\" Create dataset y = f(x1, x2, x3) + noise (r* are not used) \"\"\"\n",
    "    x1 = rand_norm(num)\n",
    "    x2 = rand_norm(num)\n",
    "    x3 = rand_norm(num)\n",
    "    n = rand_norm(num)\n",
    "    y = logit(3.0 * x1 - 2.0 * x2 + 1.0 * x3 + 0.5 * n)\n",
    "    d = {'x1': x1, 'x2': x2, 'x3': x3, 'y': y}\n",
    "    for i in range(n_rands):\n",
    "        d[f\"rand_{i}\"] = rand_norm(num)\n",
    "    df = pd.DataFrame(d)\n",
    "    if save:\n",
    "        file = 'zzz.csv'\n",
    "        print(f\"Saving dataset to file '{file}'\")\n",
    "        df.to_csv(file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_02(num=1000, n_rands=0, save=False):\n",
    "    \"\"\" Create dataset y = f(x1, x2, x3) + noise (r* are not used) \"\"\"\n",
    "    x1 = rand_norm(num)\n",
    "    x2 = rand_norm(num)\n",
    "    x3 = rand_norm(num)\n",
    "    n = rand_norm(num)\n",
    "    y = logit(5.0 * rbf(x1, 1, 1) - 3.0 * rbf(x2, -1, 1) + 1.0 * x3 + 0.5 * n)\n",
    "    d = {'x1': x1, 'x2': x2, 'x3': x3, 'y': y}\n",
    "    if n_rands > 0:\n",
    "        for i in range(n_rands):\n",
    "            d[f\"rand_{i}\"] = rand_norm(num)\n",
    "    df = pd.DataFrame(d)\n",
    "    if save:\n",
    "        file = 'zzz.csv'\n",
    "        print(f\"Saving dataset to file '{file}'\")\n",
    "        df.to_csv(file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilks_model_fit(x, y):\n",
    "    logit_model = Logit(y, x)\n",
    "    res = logit_model.fit(disp=0)\n",
    "    return logit_model, res\n",
    "\n",
    "def wilks_p_value(df, var_output, vars_null):\n",
    "    model_null, model_null_results = wilks_model_fit(df[vars_null], df[var_output])\n",
    "    pvalues = dict()\n",
    "    for c in df.columns:\n",
    "        if c != var_output and c not in vars_null:\n",
    "            xnames = list(vars_null)\n",
    "            xnames.append(c)\n",
    "            model_alt, model_alt_res = wilks_model_fit(df[xnames], df[var_output])\n",
    "            if model_alt is None:\n",
    "                self._error(f\"Could not fit alt model for column/s {c}\")\n",
    "                pval = 1.0\n",
    "            else:\n",
    "                d = 2.0 * (model_alt_res.llf - model_null_results.llf)\n",
    "                pval = chi2.sf(d, 1)\n",
    "            pvalues[c] = pval\n",
    "    return pd.Series(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance: ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validations & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def new_model(x_train, y_train):\n",
    "    return self.new_model_function(x_train, y_train)\n",
    "    model = RandomForestClassifier(100)\n",
    "    model.fit(self.x_train, self.y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "class FeatureImportanceShuffle:\n",
    "    \"\"\" Feature importance by shuffling input values \"\"\"\n",
    "    def __init__(self, model, x_train, y_train, x_val, y_val, num_iter=10):\n",
    "        self.model = model\n",
    "        self.x_train, self.y_train, self.x_val, self.y_val = x_train, y_train, x_val, y_val\n",
    "        self.num_iter = num_iter\n",
    "        self.scores_null = dict()  # Null score by variable name (one score per variable)\n",
    "        self.scores_alt = dict()  # Alt scores by variable name (multiple scores for each variable)\n",
    "\n",
    "    def feature_importance_variable(self, var):\n",
    "        \"\"\" Feature importance by shuffling variable 'var' and comparing performance results \"\"\"\n",
    "        if var not in self.scores_null:\n",
    "!!!!!!!!!            self.scores_null[var] = self.model.score(self.x_val, self.y_val)\n",
    "        # Shuffle 'var'\n",
    "        x_shuf = self.x_train.copy()\n",
    "        x_shuf[var] = x_shuf[var].sample(frac=1).values\n",
    "        # Calculate 'raw' score\n",
    "        return model.score(x_shuf, self.y_val)\n",
    "\n",
    "    def feature_importance():\n",
    "        \"\"\" Feature importance by shuffling variable 'var' and comparing performance results \"\"\"\n",
    "        for var in self.x_train.columns:\n",
    "            if var.startswith('rand_'):\n",
    "                continue\n",
    "            # Add scores list\n",
    "            if var not in self.scores:\n",
    "                self.scores[var] = list()\n",
    "            # Perform shuffling 'num_iter' times\n",
    "            for i in range(self.num_iter):\n",
    "                self.scores[var].append(self.feature_importance_variable(var))\n",
    "            scores = np.array(self.scores[var])\n",
    "            print(f\"\\t{var}:\\tcount: {len(scores)}\\tmean: {scores.mean()}\\tstd: {scores.std()}\")\n",
    "\n",
    "class FeatureImportance:\n",
    "    \"\"\"\n",
    "    Calculate feature importance.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, var_output, vars_input=None, vars_null=None, new_model_function=new_model):\n",
    "        self.df = df\n",
    "        self.var_output, self.vars_input, self.vars_null = var_output, vars_input, vars_null\n",
    "        if self.vars_input is None:\n",
    "            self.vars_input = [c for c in df.columns if c != var_output]\n",
    "        if self.vars_null is None:\n",
    "            self.vars_null = [c for c in vars_input if c.startswith('rand_')]\n",
    "        self.new_model_function = new_model_function\n",
    "        self.pvals_df = None\n",
    "        self.num_samples = 100\n",
    "        self.n_rands=5\n",
    "        self.num_cv = 1\n",
    "        self.num_shuffle = 10\n",
    "        self.cvs = list()  # List of cross validation items\n",
    "        self.models = list()  # Models by cross-validation\n",
    "        self.scores = list()  # Models scores by cross-validation\n",
    "        self.scores_null = list()  # Null models scores by cross-validation\n",
    "        \n",
    "    def new_model(self):\n",
    "        return self.new_model_function(self.x_train, self.y_train)\n",
    "    \n",
    "    def __call__():\n",
    "        # Variables\n",
    "        var_output = 'y'\n",
    "        # Create several models...\n",
    "        cv_count = 0\n",
    "        for train_index, val_index in self.cv_iter():\n",
    "            cv_count += 1\n",
    "            x_train, y_train, x_validate, y_validate = split_df(df, vars_input, var_output, train_index, val_index)\n",
    "            model = new_model(x_train, y_train)\n",
    "            # Calculate scores (shuffle)\n",
    "            scores = feature_importance_multiple_shuffle(model, x_validate, y_validate, vars_input, num_iter=num_shuffle, scores=scores)\n",
    "        # Calculate p-values\n",
    "        pvals_df = pd.DataFrame() if pvals_df is None else pvals_df\n",
    "        null_scores = np.array([scores[c] for c in vars_null]).flatten()\n",
    "        for c in vars_input:\n",
    "            null_scores = np.array([scores[cn] for cn in vars_null if cn != c]).flatten()\n",
    "            pval = sc.stats.mannwhitneyu(scores[c], null_scores, alternative='greater')[1]\n",
    "            df_row = pd.DataFrame({'name': c,\n",
    "                'num_samples': num_samples, 'n_rands': n_rands,\n",
    "                'num_cv': num_cv, 'num_shuffle': num_shuffle,\n",
    "                'count_alt': len(scores[c])\n",
    "                          , 'mean_alt': scores[c].mean(), 'std_alt': scores[c].std()\n",
    "                          , 'count_null': len(null_scores), 'mean_null': null_scores.mean()\n",
    "                          , 'std_null': null_scores.std()\n",
    "                          , 'p_value': pval}, index=[len(pvals_df)])\n",
    "            pvals_df = pvals_df.append(df_row)\n",
    "        return pvals_df\n",
    "    \n",
    "    def cv_iter(self):\n",
    "        \"\"\" Create a cross-validation iterator \"\"\"\n",
    "        if self.num_cv > 1:\n",
    "            return KFold(n_splits=self.num_cv).split(df)\n",
    "        else:\n",
    "            # No cross validation, split 80% / 20%\n",
    "            idx = int(0.8 * len(df))\n",
    "            idx_train = range(0, idx)\n",
    "            idx_val = range(idx, len(df))\n",
    "            return [(idx_train, idx_val)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def pvalues_shuffle_analysis():\n",
    "#     pvals_df = None\n",
    "#     for num_cv in [1, 3, 5, 10, 20]:\n",
    "#         for num_samples in [50, 100, 200, 300, 400, 500, 1000, 2000, 10000]:\n",
    "#             for num_shuffle in [3, 5, 10, 20, 50, 100]:\n",
    "#                 pvals_df = p_values_shuffle(num_samples=num_samples, n_rands=5, num_cv=num_cv, num_shuffle=num_shuffle, pvals_df=pvals_df)\n",
    "#                 print(f\"num_cv: {num_cv}\\tnum_samples:{num_samples}\\tnum_shuffle:{num_shuffle}\")\n",
    "#     pvals_df.to_csv('p_values_shuffle.csv')\n",
    "#     return pvals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1000\n",
    "df = create_dataset_02(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgen",
   "language": "python",
   "name": "mlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
