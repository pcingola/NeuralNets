{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene2Vec in Pytorch: Part 1\n",
    "\n",
    "\n",
    "\n",
    "We want to create embeddings using \"Gene Sets\" from MsiDb.\n",
    "The idea is to create an encoder: gene -> geneSets, then use the middle layer as en embedding to represent genes.\n",
    "\n",
    "### Network\n",
    "\n",
    "- Input (genes): One-hot encoded. There are 23,112 genes in MsigDb sets we are interested in, so the inputs are vectors of dimention 23,112 where all inputs are zero, except one (one-hot)\n",
    "- Hidden layer 1: Converts the input from 23,112 to lower dimension vector (e.g. 100), this is the \"Embedding\" we'll use\n",
    "- Hidden layer 2: Linear + Relu\n",
    "- Output (gene sets): Many-hot encoded, because one gene can belong to multiple gene sets (i.e. this is a Multi-label classification problem). There are 20,608 gene sets, so the output is a vector of dimension 20,608 and most of the outputs should be zero.\n",
    "- Output non-linearity: We use sigmoid function. Why not something like 'sofmax'? Because this is a multi-label classification problem, the output one neuron should not be influenced by the output of the other neurons (i.e. the sum of the outputs doesn't have to be 1.0) \n",
    "- Loss function: Obviously if we use MSE this won't work well because an easy way to lower the MSE is to make all the outputs zero (most of the outputs are 0.0, expcet for a few 1.0). In multi-label classification an typical loss function is binary-cross-entropy (the intuition is that it treats each output neuron as a binary classification problem).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tcga.msigdb import *\n",
    "from tcga.util import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We read the MsigDb and produce a 'dataset' with inputs and outputs.\n",
    "- Inputs: Genes as one-hot vectors, e.g. `[0, 0, ..., 0, 1, 0, ..., 0]`\n",
    "- Outputs: GeneSets as many-hot vectors, e.g. `[0, 1, ..., 0, 1, 1, ..., 0, 1, 0]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names2num(names):\n",
    "    \"\"\" Create a mapping from names to numbers \"\"\"\n",
    "    names = list(set(names))  # Make sure names are unique\n",
    "    return {n: i for i,n in enumerate(sorted(names))}\n",
    "    \n",
    "class DatasetMsigDb(Dataset):\n",
    "    \"\"\" \n",
    "    Custom dataset: We have to override methods __len__ and __getitem__\n",
    "    In our network, the inputs are genes and the outputs are gene-sets.\n",
    "    We convert genes and gene sets to numbers, then store the forward and\n",
    "    reverse mappings.\n",
    "    \n",
    "    Genes: One hot encoding\n",
    "    \n",
    "    Gene sets: We encode gene-> gene_sets as a dictionary indexed by\n",
    "    gene, with tensors having 1 on the GeneSets the gene belongs to.\n",
    "    \n",
    "    The method __getitem__ returns a tuple with the gene (one-hot\n",
    "    encoded) and the gene-set tensor (having ones on all gene-sets\n",
    "    the gene belongs to)\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.msigdb = read_msigdb_all(path)\n",
    "        # Gene <-> number: forward and reverse mapping\n",
    "        self.gene2num = names2num(msigdb2genes(self.msigdb))\n",
    "        self.num2gene = {n: g for g, n in self.gene2num.items()}\n",
    "        # GeneSet <-> number: forward and reverse mapping\n",
    "        self.geneset2num = names2num(msigdb2gene_sets(self.msigdb))\n",
    "        self.num2geneset = {n: g for g, n in self.gene2num.items()}\n",
    "        # Gene -> GeneSets mapping (use gene_set numbers, in a tensor)\n",
    "        self.init_genes()\n",
    "        self.init_genesets()\n",
    "\n",
    "    def genesets2num(self, genesets):\n",
    "        \" Convert to a list of numerically encoded gene-sets \"\n",
    "        return [self.geneset2num[gs] for gs in genesets]\n",
    "\n",
    "    def gene2tensor(self, gene):\n",
    "        \" Convert to a one-hot encoding \"\n",
    "        gene_tensor = torch.zeros(len(self.gene2num))\n",
    "        gene_tensor[self.gene2num[gene]] = 1\n",
    "        return gene_tensor\n",
    "        \n",
    "    def genesets2tensor(self, genesets):\n",
    "        \" Convert to a vector having 1 in each geneset position \"\n",
    "        geneset_idxs = [self.geneset2num[gs] for gs in genesets]\n",
    "        geneset_tensor = torch.zeros(len(self.msigdb))\n",
    "        geneset_tensor[geneset_idxs] = 1\n",
    "        return geneset_tensor\n",
    "        \n",
    "    def init_genes(self):\n",
    "        \" Create a one-hot encoding for a gene \"\n",
    "        self.gene_tensors = dict()\n",
    "        for gene in self.gene2num.keys():\n",
    "            self.gene_tensors[gene] = self.gene2tensor(gene)\n",
    "        \n",
    "    def init_genesets(self):\n",
    "        \" Map Gene to GeneSets. GeneSets are hot-encoded \"\n",
    "        self.gene_genesets = dict()\n",
    "        self.gene_genesets_num = dict()\n",
    "        self.gene_genesets_tensors = dict()\n",
    "        num_genesets = len(self.geneset2num)\n",
    "        for gene, genesets in gene_genesets(self.msigdb).items():\n",
    "            self.gene_genesets[gene] = genesets\n",
    "            self.gene_genesets_num[gene] = self.genesets2num(genesets)\n",
    "            self.gene_genesets_tensors[gene] = self.genesets2tensor(genesets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \" Len: Count number of genes \"\n",
    "        return len(self.gene2num)\n",
    "\n",
    "    def gene_sets_size(self):\n",
    "        \" Count number of gene sets \"\n",
    "        return len(self.msigdb)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \" Get item 'idx': A tuple of gene number 'idx' and gene set tensor for that gene \"\n",
    "        gene = self.num2gene[idx]\n",
    "        return (self.gene_tensors[gene], self.gene_genesets_tensors[gene])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        \" Show (a few) mappings gene -> gene_set tensor \"\n",
    "        out = f\"Genes: {len(self.gene2num)}, Gene Sets: {len(self.geneset2num)}\\n\"\n",
    "        for i in range(10):  #range(len(self)):\n",
    "            gene = self.num2gene[i]\n",
    "            gene_tensor, geneset_tensor = self[i]\n",
    "            out += f\"\\tGene: {gene}, {i}, {gene_tensor}\\n\\tGeneSet: {self.gene_genesets[gene]}, {self.gene_genesets_num[gene]}, {geneset_tensor}\\n\\n\"\n",
    "        return out + \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/msigdb/small/h.all.v7.0.symbols.gmt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Genes: 4384, Gene Sets: 50\n",
       "\tGene: A2M, 0, tensor([1., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_IL6_JAK_STAT3_SIGNALING', 'HALLMARK_COAGULATION'}, [23, 9], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AAAS, 1, tensor([0., 1., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_DNA_REPAIR'}, [11], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AADAT, 2, tensor([0., 0., 1.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_FATTY_ACID_METABOLISM'}, [16], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AARS, 3, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_ALLOGRAFT_REJECTION'}, [1], tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABAT, 4, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_P53_PATHWAY', 'HALLMARK_ESTROGEN_RESPONSE_EARLY'}, [36, 14], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA1, 5, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_TNFA_SIGNALING_VIA_NFKB', 'HALLMARK_PROTEIN_SECRETION', 'HALLMARK_ADIPOGENESIS', 'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_INFLAMMATORY_RESPONSE'}, [44, 40, 0, 7, 24], tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA2, 6, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_CHOLESTEROL_HOMEOSTASIS'}, [7, 8], tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA3, 7, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_ESTROGEN_RESPONSE_EARLY', 'HALLMARK_ESTROGEN_RESPONSE_LATE'}, [7, 14, 15], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA4, 8, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA5, 9, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('data/msigdb/small')\n",
    "dataset = DatasetMsigDb(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene2GeneSetModule(nn.Module):\n",
    "    def __init__(self, dataset_msigdb, embedding_dim, layer_size=10):\n",
    "        super(Gene2GeneSetModule, self).__init__()\n",
    "        genes_vocab_size = len(dataset_msigdb)\n",
    "        genesets_vocab_size = dataset_msigdb.gene_sets_size()\n",
    "        self.embeddings = nn.Linear(genes_vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, layer_size)\n",
    "        self.linear2 = nn.Linear(layer_size, genesets_vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.embeddings(inputs))\n",
    "        x = F.relu(self.linear1(x))\n",
    "        probs = torch.sigmoid(self.linear2(x))\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, lr, momentum):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for n_epoch in range(epochs):\n",
    "        for n_batch, batch in enumerate(dataloader):\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = F.binary_cross_entropy(output.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if n_epoch % 1 == 0 and n_batch == 0:\n",
    "                print(f\"Train Epoch: {n_epoch} / {epochs}\\tn_batch: {n_batch}\\tLoss: {loss.item():.6f}\\tx.shape: {x.shape}\\ty.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 / 100\tn_batch: 0\tLoss: 0.707272\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 1 / 100\tn_batch: 0\tLoss: 0.671602\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 2 / 100\tn_batch: 0\tLoss: 0.627928\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 3 / 100\tn_batch: 0\tLoss: 0.562565\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 4 / 100\tn_batch: 0\tLoss: 0.459402\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 5 / 100\tn_batch: 0\tLoss: 0.321267\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 6 / 100\tn_batch: 0\tLoss: 0.197739\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 7 / 100\tn_batch: 0\tLoss: 0.152121\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 8 / 100\tn_batch: 0\tLoss: 0.159890\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 9 / 100\tn_batch: 0\tLoss: 0.162811\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 10 / 100\tn_batch: 0\tLoss: 0.153488\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 11 / 100\tn_batch: 0\tLoss: 0.143689\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 12 / 100\tn_batch: 0\tLoss: 0.140026\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 13 / 100\tn_batch: 0\tLoss: 0.140307\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 14 / 100\tn_batch: 0\tLoss: 0.140208\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 15 / 100\tn_batch: 0\tLoss: 0.139005\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 16 / 100\tn_batch: 0\tLoss: 0.138038\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 17 / 100\tn_batch: 0\tLoss: 0.137743\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 18 / 100\tn_batch: 0\tLoss: 0.137636\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 19 / 100\tn_batch: 0\tLoss: 0.137420\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 20 / 100\tn_batch: 0\tLoss: 0.137201\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 21 / 100\tn_batch: 0\tLoss: 0.137070\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 22 / 100\tn_batch: 0\tLoss: 0.136917\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 23 / 100\tn_batch: 0\tLoss: 0.136663\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 24 / 100\tn_batch: 0\tLoss: 0.136280\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 25 / 100\tn_batch: 0\tLoss: 0.135819\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 26 / 100\tn_batch: 0\tLoss: 0.135275\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 27 / 100\tn_batch: 0\tLoss: 0.134622\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 28 / 100\tn_batch: 0\tLoss: 0.133866\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 29 / 100\tn_batch: 0\tLoss: 0.133045\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 30 / 100\tn_batch: 0\tLoss: 0.132184\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 31 / 100\tn_batch: 0\tLoss: 0.131346\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 32 / 100\tn_batch: 0\tLoss: 0.130546\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 33 / 100\tn_batch: 0\tLoss: 0.129726\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 34 / 100\tn_batch: 0\tLoss: 0.128891\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 35 / 100\tn_batch: 0\tLoss: 0.128059\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 36 / 100\tn_batch: 0\tLoss: 0.127249\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 37 / 100\tn_batch: 0\tLoss: 0.126456\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 38 / 100\tn_batch: 0\tLoss: 0.125654\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 39 / 100\tn_batch: 0\tLoss: 0.124848\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 40 / 100\tn_batch: 0\tLoss: 0.124031\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 41 / 100\tn_batch: 0\tLoss: 0.123191\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 42 / 100\tn_batch: 0\tLoss: 0.122323\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 43 / 100\tn_batch: 0\tLoss: 0.121405\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 44 / 100\tn_batch: 0\tLoss: 0.120430\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 45 / 100\tn_batch: 0\tLoss: 0.119379\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 46 / 100\tn_batch: 0\tLoss: 0.118247\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 47 / 100\tn_batch: 0\tLoss: 0.117032\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 48 / 100\tn_batch: 0\tLoss: 0.115729\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 49 / 100\tn_batch: 0\tLoss: 0.114329\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 50 / 100\tn_batch: 0\tLoss: 0.112851\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 51 / 100\tn_batch: 0\tLoss: 0.111273\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 52 / 100\tn_batch: 0\tLoss: 0.109530\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 53 / 100\tn_batch: 0\tLoss: 0.107384\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 54 / 100\tn_batch: 0\tLoss: 0.105227\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 55 / 100\tn_batch: 0\tLoss: 0.103164\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 56 / 100\tn_batch: 0\tLoss: 0.101165\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 57 / 100\tn_batch: 0\tLoss: 0.099218\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 58 / 100\tn_batch: 0\tLoss: 0.097300\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 59 / 100\tn_batch: 0\tLoss: 0.095303\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 60 / 100\tn_batch: 0\tLoss: 0.093244\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 61 / 100\tn_batch: 0\tLoss: 0.091110\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 62 / 100\tn_batch: 0\tLoss: 0.088914\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 63 / 100\tn_batch: 0\tLoss: 0.086666\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 64 / 100\tn_batch: 0\tLoss: 0.084376\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 65 / 100\tn_batch: 0\tLoss: 0.082110\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 66 / 100\tn_batch: 0\tLoss: 0.079777\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 67 / 100\tn_batch: 0\tLoss: 0.077578\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 68 / 100\tn_batch: 0\tLoss: 0.075453\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 69 / 100\tn_batch: 0\tLoss: 0.073340\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 70 / 100\tn_batch: 0\tLoss: 0.071474\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 71 / 100\tn_batch: 0\tLoss: 0.069702\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 72 / 100\tn_batch: 0\tLoss: 0.068014\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 73 / 100\tn_batch: 0\tLoss: 0.066506\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 74 / 100\tn_batch: 0\tLoss: 0.065086\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 75 / 100\tn_batch: 0\tLoss: 0.063780\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 76 / 100\tn_batch: 0\tLoss: 0.062576\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 77 / 100\tn_batch: 0\tLoss: 0.061500\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 78 / 100\tn_batch: 0\tLoss: 0.060442\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 79 / 100\tn_batch: 0\tLoss: 0.059402\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 80 / 100\tn_batch: 0\tLoss: 0.058556\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 81 / 100\tn_batch: 0\tLoss: 0.057738\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 82 / 100\tn_batch: 0\tLoss: 0.056828\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 83 / 100\tn_batch: 0\tLoss: 0.055990\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 84 / 100\tn_batch: 0\tLoss: 0.055260\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 85 / 100\tn_batch: 0\tLoss: 0.054523\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 86 / 100\tn_batch: 0\tLoss: 0.053810\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 87 / 100\tn_batch: 0\tLoss: 0.053138\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 88 / 100\tn_batch: 0\tLoss: 0.052496\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 89 / 100\tn_batch: 0\tLoss: 0.051894\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 90 / 100\tn_batch: 0\tLoss: 0.051279\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 91 / 100\tn_batch: 0\tLoss: 0.050677\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 92 / 100\tn_batch: 0\tLoss: 0.050136\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 93 / 100\tn_batch: 0\tLoss: 0.049653\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 94 / 100\tn_batch: 0\tLoss: 0.049116\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 95 / 100\tn_batch: 0\tLoss: 0.048660\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 96 / 100\tn_batch: 0\tLoss: 0.048234\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 97 / 100\tn_batch: 0\tLoss: 0.047683\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 98 / 100\tn_batch: 0\tLoss: 0.047184\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 99 / 100\tn_batch: 0\tLoss: 0.046771\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1000)\n",
    "model = Gene2GeneSetModule(dataset, 20, 10)\n",
    "train(model, dataloader, epochs=100, lr=0.01, momentum=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgen",
   "language": "python",
   "name": "mlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
