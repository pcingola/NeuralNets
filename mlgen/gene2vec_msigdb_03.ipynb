{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene2Vec: Pytorch\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "https://discuss.pytorch.org/t/what-kind-of-loss-is-better-to-use-in-multilabel-classification/32203\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tcga.msigdb import *\n",
    "from tcga.util import *\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names2num(names):\n",
    "    \"\"\" Create a mapping from names to numbers \"\"\"\n",
    "    names = list(set(names))  # Make sure names are unique\n",
    "    return {n: i for i,n in enumerate(sorted(names))}\n",
    "    \n",
    "class DatasetMsigDb(Dataset):\n",
    "    \"\"\" \n",
    "    Custom dataset: We have to override methods __len__ and __getitem__\n",
    "    In our network, the inputs are genes and the outputs are gene-sets.\n",
    "    We convert genes and gene sets to numbers, then store the forward and\n",
    "    reverse mappings.\n",
    "    \n",
    "    Genes: One hot encoding\n",
    "    \n",
    "    Gene sets: We encode gene-> gene_sets as a dictionary indexed by\n",
    "    gene, with tensors having 1 on the GeneSets the gene belongs to.\n",
    "    \n",
    "    The method __getitem__ returns a tuple with the gene (one-hot\n",
    "    encoded) and the gene-set tensor (having ones on all gene-sets\n",
    "    the gene belongs to)\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.msigdb = read_msigdb_all(path)\n",
    "        # Gene <-> number: forward and reverse mapping\n",
    "        self.gene2num = names2num(msigdb2genes(self.msigdb))\n",
    "        self.num2gene = {n: g for g, n in self.gene2num.items()}\n",
    "        # GeneSet <-> number: forward and reverse mapping\n",
    "        self.geneset2num = names2num(msigdb2gene_sets(self.msigdb))\n",
    "        self.num2geneset = {n: g for g, n in self.gene2num.items()}\n",
    "        # Gene -> GeneSets mapping (use gene_set numbers, in a tensor)\n",
    "        self.init_genes()\n",
    "        self.init_genesets()\n",
    "\n",
    "    def genesets2num(self, genesets):\n",
    "        \" Convert to a list of numerically encoded gene-sets \"\n",
    "        return [self.geneset2num[gs] for gs in genesets]\n",
    "\n",
    "    def gene2tensor(self, gene):\n",
    "        \" Convert to a one-hot encoding \"\n",
    "        gene_tensor = torch.zeros(len(self.gene2num))\n",
    "        gene_tensor[self.gene2num[gene]] = 1\n",
    "        return gene_tensor\n",
    "        \n",
    "    def genesets2tensor(self, genesets):\n",
    "        \" Convert to a vector having 1 in each geneset position \"\n",
    "        geneset_idxs = [self.geneset2num[gs] for gs in genesets]\n",
    "        geneset_tensor = torch.zeros(len(self.msigdb))\n",
    "        geneset_tensor[geneset_idxs] = 1\n",
    "        return geneset_tensor\n",
    "        \n",
    "    def init_genes(self):\n",
    "        \" Create a one-hot encoding for a gene \"\n",
    "        self.gene_tensors = dict()\n",
    "        for gene in self.gene2num.keys():\n",
    "            self.gene_tensors[gene] = self.gene2tensor(gene)\n",
    "        \n",
    "    def init_genesets(self):\n",
    "        \" Map Gene to GeneSets. GeneSets are hot-encoded \"\n",
    "        self.gene_genesets = dict()\n",
    "        self.gene_genesets_num = dict()\n",
    "        self.gene_genesets_tensors = dict()\n",
    "        num_genesets = len(self.geneset2num)\n",
    "        for gene, genesets in gene_genesets(self.msigdb).items():\n",
    "            self.gene_genesets[gene] = genesets\n",
    "            self.gene_genesets_num[gene] = self.genesets2num(genesets)\n",
    "            self.gene_genesets_tensors[gene] = self.genesets2tensor(genesets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \" Len: Count number of genes \"\n",
    "        return len(self.gene2num)\n",
    "\n",
    "    def gene_sets_size(self):\n",
    "        \" Count number of gene sets \"\n",
    "        return len(self.msigdb)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \" Get item 'idx': A tuple of gene number 'idx' and gene set tensor for that gene \"\n",
    "        gene = self.num2gene[idx]\n",
    "        return (self.gene_tensors[gene], self.gene_genesets_tensors[gene])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        \" Show (a few) mappings gene -> gene_set tensor \"\n",
    "        out = f\"Genes: {len(self.gene2num)}, Gene Sets: {len(self.geneset2num)}\\n\"\n",
    "        for i in range(10):  #range(len(self)):\n",
    "            gene = self.num2gene[i]\n",
    "            gene_tensor, geneset_tensor = self[i]\n",
    "            out += f\"\\tGene: {gene}, {i}, {gene_tensor}\\n\\tGeneSet: {self.gene_genesets[gene]}, {self.gene_genesets_num[gene]}, {geneset_tensor}\\n\\n\"\n",
    "        return out + \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/msigdb/small/h.all.v7.0.symbols.gmt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Genes: 4384, Gene Sets: 50\n",
       "\tGene: A2M, 0, tensor([1., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_IL6_JAK_STAT3_SIGNALING', 'HALLMARK_COAGULATION'}, [23, 9], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AAAS, 1, tensor([0., 1., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_DNA_REPAIR'}, [11], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AADAT, 2, tensor([0., 0., 1.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_FATTY_ACID_METABOLISM'}, [16], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: AARS, 3, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_ALLOGRAFT_REJECTION'}, [1], tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABAT, 4, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_P53_PATHWAY', 'HALLMARK_ESTROGEN_RESPONSE_EARLY'}, [36, 14], tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA1, 5, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_TNFA_SIGNALING_VIA_NFKB', 'HALLMARK_PROTEIN_SECRETION', 'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_INFLAMMATORY_RESPONSE', 'HALLMARK_ADIPOGENESIS'}, [44, 40, 7, 24, 0], tensor([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA2, 6, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_CHOLESTEROL_HOMEOSTASIS', 'HALLMARK_BILE_ACID_METABOLISM'}, [8, 7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA3, 7, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_ESTROGEN_RESPONSE_LATE', 'HALLMARK_BILE_ACID_METABOLISM', 'HALLMARK_ESTROGEN_RESPONSE_EARLY'}, [15, 7, 14], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA4, 8, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "\tGene: ABCA5, 9, tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
       "\tGeneSet: {'HALLMARK_BILE_ACID_METABOLISM'}, [7], tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "\n",
       "..."
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('data/msigdb/small')\n",
    "dataset = DatasetMsigDb(path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene2GeneSetModule(nn.Module):\n",
    "    def __init__(self, dataset_msigdb, embedding_dim, layer_size=10):\n",
    "        super(Gene2GeneSetModule, self).__init__()\n",
    "        genes_vocab_size = len(dataset_msigdb)\n",
    "        genesets_vocab_size = dataset_msigdb.gene_sets_size()\n",
    "        # self.embeddings = nn.Embedding(genes_vocab_size, embedding_dim)\n",
    "        self.embeddings = nn.Linear(genes_vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, layer_size)\n",
    "        self.linear2 = nn.Linear(layer_size, genesets_vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.embeddings(inputs))\n",
    "        x = F.relu(self.linear1(x))\n",
    "        probs = torch.sigmoid(self.linear2(x))\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs, lr, momentum):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for n_epoch in range(epochs):\n",
    "        for n_batch, batch in enumerate(dataloader):\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = F.binary_cross_entropy(output.squeeze(), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if n_epoch % 1 == 0 and n_batch == 0:\n",
    "                print(f\"Train Epoch: {n_epoch} / {epochs}\\tn_batch: {n_batch}\\tLoss: {loss.item():.6f}\\tx.shape: {x.shape}\\ty.shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 / 100\tn_batch: 0\tLoss: 0.696408\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 1 / 100\tn_batch: 0\tLoss: 0.660038\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 2 / 100\tn_batch: 0\tLoss: 0.620970\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 3 / 100\tn_batch: 0\tLoss: 0.563703\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 4 / 100\tn_batch: 0\tLoss: 0.477061\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 5 / 100\tn_batch: 0\tLoss: 0.360194\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 6 / 100\tn_batch: 0\tLoss: 0.236580\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 7 / 100\tn_batch: 0\tLoss: 0.162840\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 8 / 100\tn_batch: 0\tLoss: 0.154340\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 9 / 100\tn_batch: 0\tLoss: 0.160283\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 10 / 100\tn_batch: 0\tLoss: 0.154521\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 11 / 100\tn_batch: 0\tLoss: 0.144366\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 12 / 100\tn_batch: 0\tLoss: 0.140173\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 13 / 100\tn_batch: 0\tLoss: 0.140496\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 14 / 100\tn_batch: 0\tLoss: 0.139555\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 15 / 100\tn_batch: 0\tLoss: 0.138501\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 16 / 100\tn_batch: 0\tLoss: 0.138098\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 17 / 100\tn_batch: 0\tLoss: 0.137783\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 18 / 100\tn_batch: 0\tLoss: 0.137357\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 19 / 100\tn_batch: 0\tLoss: 0.137014\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 20 / 100\tn_batch: 0\tLoss: 0.136828\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 21 / 100\tn_batch: 0\tLoss: 0.136667\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 22 / 100\tn_batch: 0\tLoss: 0.136479\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 23 / 100\tn_batch: 0\tLoss: 0.136295\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 24 / 100\tn_batch: 0\tLoss: 0.136092\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 25 / 100\tn_batch: 0\tLoss: 0.135831\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 26 / 100\tn_batch: 0\tLoss: 0.135503\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 27 / 100\tn_batch: 0\tLoss: 0.135100\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 28 / 100\tn_batch: 0\tLoss: 0.134604\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 29 / 100\tn_batch: 0\tLoss: 0.133988\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 30 / 100\tn_batch: 0\tLoss: 0.133231\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 31 / 100\tn_batch: 0\tLoss: 0.132275\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 32 / 100\tn_batch: 0\tLoss: 0.131104\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 33 / 100\tn_batch: 0\tLoss: 0.129677\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 34 / 100\tn_batch: 0\tLoss: 0.128003\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 35 / 100\tn_batch: 0\tLoss: 0.126308\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 36 / 100\tn_batch: 0\tLoss: 0.124718\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 37 / 100\tn_batch: 0\tLoss: 0.123116\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 38 / 100\tn_batch: 0\tLoss: 0.121469\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 39 / 100\tn_batch: 0\tLoss: 0.119783\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 40 / 100\tn_batch: 0\tLoss: 0.118078\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 41 / 100\tn_batch: 0\tLoss: 0.116298\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 42 / 100\tn_batch: 0\tLoss: 0.114252\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 43 / 100\tn_batch: 0\tLoss: 0.112082\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 44 / 100\tn_batch: 0\tLoss: 0.109764\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 45 / 100\tn_batch: 0\tLoss: 0.107382\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 46 / 100\tn_batch: 0\tLoss: 0.104939\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 47 / 100\tn_batch: 0\tLoss: 0.102412\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 48 / 100\tn_batch: 0\tLoss: 0.099951\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 49 / 100\tn_batch: 0\tLoss: 0.097499\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 50 / 100\tn_batch: 0\tLoss: 0.095020\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 51 / 100\tn_batch: 0\tLoss: 0.092574\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 52 / 100\tn_batch: 0\tLoss: 0.090155\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 53 / 100\tn_batch: 0\tLoss: 0.087793\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 54 / 100\tn_batch: 0\tLoss: 0.085583\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 55 / 100\tn_batch: 0\tLoss: 0.083488\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 56 / 100\tn_batch: 0\tLoss: 0.081432\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 57 / 100\tn_batch: 0\tLoss: 0.079691\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 58 / 100\tn_batch: 0\tLoss: 0.077789\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 59 / 100\tn_batch: 0\tLoss: 0.075821\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 60 / 100\tn_batch: 0\tLoss: 0.074170\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 61 / 100\tn_batch: 0\tLoss: 0.072320\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 62 / 100\tn_batch: 0\tLoss: 0.070397\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 63 / 100\tn_batch: 0\tLoss: 0.068713\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 64 / 100\tn_batch: 0\tLoss: 0.068126\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 65 / 100\tn_batch: 0\tLoss: 0.064796\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 66 / 100\tn_batch: 0\tLoss: 0.062549\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 67 / 100\tn_batch: 0\tLoss: 0.060399\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 68 / 100\tn_batch: 0\tLoss: 0.058130\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 69 / 100\tn_batch: 0\tLoss: 0.055896\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 70 / 100\tn_batch: 0\tLoss: 0.053744\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 71 / 100\tn_batch: 0\tLoss: 0.051877\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 72 / 100\tn_batch: 0\tLoss: 0.050298\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 73 / 100\tn_batch: 0\tLoss: 0.048843\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 74 / 100\tn_batch: 0\tLoss: 0.047670\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 75 / 100\tn_batch: 0\tLoss: 0.046468\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 76 / 100\tn_batch: 0\tLoss: 0.045415\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 77 / 100\tn_batch: 0\tLoss: 0.044514\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 78 / 100\tn_batch: 0\tLoss: 0.043807\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 79 / 100\tn_batch: 0\tLoss: 0.043033\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 80 / 100\tn_batch: 0\tLoss: 0.042129\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 81 / 100\tn_batch: 0\tLoss: 0.041539\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 82 / 100\tn_batch: 0\tLoss: 0.041161\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 83 / 100\tn_batch: 0\tLoss: 0.040146\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 84 / 100\tn_batch: 0\tLoss: 0.039415\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 85 / 100\tn_batch: 0\tLoss: 0.038796\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 86 / 100\tn_batch: 0\tLoss: 0.038236\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 87 / 100\tn_batch: 0\tLoss: 0.037699\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 88 / 100\tn_batch: 0\tLoss: 0.037205\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 89 / 100\tn_batch: 0\tLoss: 0.036732\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 90 / 100\tn_batch: 0\tLoss: 0.036276\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 91 / 100\tn_batch: 0\tLoss: 0.035816\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 92 / 100\tn_batch: 0\tLoss: 0.035414\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 93 / 100\tn_batch: 0\tLoss: 0.035107\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 94 / 100\tn_batch: 0\tLoss: 0.034730\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 95 / 100\tn_batch: 0\tLoss: 0.034245\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 96 / 100\tn_batch: 0\tLoss: 0.033984\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 97 / 100\tn_batch: 0\tLoss: 0.033939\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 98 / 100\tn_batch: 0\tLoss: 0.033501\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n",
      "Train Epoch: 99 / 100\tn_batch: 0\tLoss: 0.032920\tx.shape: torch.Size([1000, 4384])\ty.shape: torch.Size([1000, 50])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1000)\n",
    "model = Gene2GeneSetModule(dataset, 20, 10)\n",
    "train(model, dataloader, epochs=100, lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4384]), torch.Size([1, 50]))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = batch\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor(2.))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9600e-02, 2.5676e-05, 1.5609e-11, 1.2241e-05, 1.0953e-09, 9.6507e-05,\n",
       "         1.4133e-05, 3.4642e-04, 1.6628e-06, 8.2783e-01, 7.5926e-02, 2.1992e-06,\n",
       "         6.1009e-12, 4.6155e-06, 1.5533e-04, 3.2543e-02, 4.0315e-04, 1.6284e-13,\n",
       "         1.3446e-08, 1.4023e-08, 9.9435e-14, 8.8443e-07, 4.1290e-02, 7.2486e-01,\n",
       "         5.9077e-05, 3.2605e-02, 7.3728e-03, 7.3745e-06, 3.7296e-02, 1.8355e-11,\n",
       "         8.1447e-10, 3.9899e-13, 6.5733e-14, 4.6673e-07, 9.2048e-08, 3.3211e-04,\n",
       "         6.2012e-05, 3.1749e-06, 1.4756e-02, 1.1596e-08, 1.3772e-08, 5.0771e-07,\n",
       "         9.8594e-09, 3.7925e-09, 8.1214e-04, 1.0957e-08, 5.6726e-13, 5.8267e-06,\n",
       "         1.0721e-08, 6.6117e-02]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]], dtype=torch.int32),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.8278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7249, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output > 0.1).int(), y, output * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0057,  0.0155,  0.0148,  0.0012,  0.0084,  0.0125,  0.0310,  0.0058,\n",
       "          0.0028, -0.8903,  0.0162,  0.0179,  0.0104,  0.0032,  0.0119,  0.0161,\n",
       "          0.0051,  0.0193,  0.0099,  0.0033,  0.0083,  0.0027,  0.0038, -0.8947,\n",
       "          0.0139,  0.0144,  0.0254,  0.0361,  0.0181,  0.0065,  0.0090,  0.0129,\n",
       "          0.0037,  0.0046,  0.0317,  0.0083,  0.0154,  0.0067,  0.0058,  0.0038,\n",
       "          0.0071,  0.0324,  0.0064,  0.0208,  0.0104,  0.0245,  0.0063,  0.0212,\n",
       "          0.0092,  0.0064]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6591, -0.1252,  0.6525,  0.6117,  0.1725, -0.1092,  0.5916,  0.6021,\n",
       "          0.3134,  0.4587, -0.0986,  0.1905,  0.6319,  0.6196, -0.1314,  0.3249,\n",
       "         -0.1442, -0.0804,  0.6251,  0.6490]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he = model.embeddings(x)\n",
    "he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6591, 0.0000, 0.6525, 0.6117, 0.1725, 0.0000, 0.5916, 0.6021, 0.3134,\n",
       "         0.4587, 0.0000, 0.1905, 0.6319, 0.6196, 0.0000, 0.3249, 0.0000, 0.0000,\n",
       "         0.6251, 0.6490]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe = F.relu(he)\n",
    "xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2316, -0.4473, -1.4305, -0.3557,  2.9244,  3.2764, -0.7536, -0.3363,\n",
       "          3.9937, -0.1228]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl1 = model.linear1(xe)\n",
    "hl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 2.9244, 3.2764, 0.0000, 0.0000, 3.9937,\n",
       "         0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl1 = F.relu(hl1)\n",
    "xl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1570, -4.1510, -4.1982, -6.6995, -4.7714, -4.3663, -3.4408, -5.1393,\n",
       "         -5.8730, -4.6303, -4.1082, -4.0032, -4.5511, -5.7505, -4.4198, -4.1140,\n",
       "         -5.2817, -3.9262, -4.6030, -5.6990, -4.7828, -5.9047, -5.5611, -5.2333,\n",
       "         -4.2636, -4.2290, -3.6491, -3.2849, -3.9926, -5.0236, -4.6992, -4.3374,\n",
       "         -5.5866, -5.3676, -3.4187, -4.7862, -4.1592, -5.0032, -5.1436, -5.5789,\n",
       "         -4.9371, -3.3968, -5.0464, -3.8539, -4.5558, -3.6838, -5.0565, -3.8329,\n",
       "         -4.6815, -5.0523]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl2 = model.linear2(xl1)\n",
    "hl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0057, 0.0155, 0.0148, 0.0012, 0.0084, 0.0125, 0.0310, 0.0058, 0.0028,\n",
       "         0.0097, 0.0162, 0.0179, 0.0104, 0.0032, 0.0119, 0.0161, 0.0051, 0.0193,\n",
       "         0.0099, 0.0033, 0.0083, 0.0027, 0.0038, 0.0053, 0.0139, 0.0144, 0.0254,\n",
       "         0.0361, 0.0181, 0.0065, 0.0090, 0.0129, 0.0037, 0.0046, 0.0317, 0.0083,\n",
       "         0.0154, 0.0067, 0.0058, 0.0038, 0.0071, 0.0324, 0.0064, 0.0208, 0.0104,\n",
       "         0.0245, 0.0063, 0.0212, 0.0092, 0.0064]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(hl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgen",
   "language": "python",
   "name": "mlgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
